{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "497b74ed0b990037",
   "metadata": {},
   "source": [
    "# Lab on Prompting and fine-tuning\n",
    "\n",
    "In this lab, we will cover the basics that are needed to perform the homework 2 on Prompting and Fine-Tuning LLMs. As you will see in the next section, you will see that the order of the lab is sligthly different from the HW. This is intentional, as the fine-tuning of a model will take additional time, whereas the Few-Shot exercise (2) builds on the knowledge of exercise 1.\n",
    "\n",
    "## Layout of the Lab\n",
    "\n",
    "We will cover the following in the first part;\n",
    "\n",
    "1. Preparation of the `model` and `dataset`,\n",
    "2. Pre-processing of the `dataset`, using `jinja2` templating,\n",
    "3. Running inference experiments.\n",
    "\n",
    "In the second part, we will focus on;\n",
    "\n",
    "1. Fine-Tuning a model,\n",
    "2. Hyper-parameters to consider,\n",
    "3. Running inference with a fine-tuned model.\n",
    "\n",
    "In the third part, which will likely be mostly the 2nd lab (next week), focusses on ;\n",
    "1. Few-shot Learning,\n",
    "2. Creating context,\n",
    "3. Running inference experiemnts.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d16bf6f9c4752f",
   "metadata": {},
   "source": [
    "## Installing dependencies\n",
    "Just to be sure, run the following to install (any missing) dependencies. You only have to run this once if you have persistent storage for you `venv` or `conda` environment. However, jsut to be shure, you might want to run this again. \n",
    "\n",
    "> Depending on your bandwith, disk, and CPU this might take a while.\n",
    "\n",
    "## Change log\n",
    "* 23 Oct 2024: we have updated the dependencies to include `protobuf` needed to load certain models.\n",
    "* 23 Oct 2024: we have updated the Part 1 & 2 to address minor issues in the code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ffb0dfeb61e459",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Install the packages, if you run this a second time with persistent storage, feel free to skip.\n",
    "%pip install numpy~=1.26.0 torch~=2.2.1 transformers accelerate datasets bitsandbytes sentencepiece peft accelerate nbconvert==6.5.4 pypdf2==2 \"lxml[html_clean]\" notebook-as-pdf seaborn protobuf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ba03fea5ba1506",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# This will be (most) of the packages that you will need during the lab.\n",
    "# Make sure to run this cell each time you (re-)start the IPython kernel.\n",
    "import textwrap\n",
    "import warnings\n",
    "from importlib import metadata\n",
    "\n",
    "import datasets\n",
    "import jinja2\n",
    "from typing import *\n",
    "from collections import defaultdict\n",
    "import itertools\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "from IPython.display import display, HTML, Markdown\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "print(\"Imports done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6965ab32696e4800",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "def display_dataset_description(name: str, dataset: datasets.DatasetDict):\n",
    "    split_info = []\n",
    "    for k, ds in dataset.items():\n",
    "        split_info.append(f\"<tr><td><strong>{k.capitalize()} Samples:</strong></td><td>{len(ds)}</td></tr>\")\n",
    "    html_content = f\"\"\"\n",
    "    <h2>Dataset info</h2>\n",
    "    <table>\n",
    "        <tr><td><strong>Dataset Name:</strong></td><td>{name}</td></tr>\n",
    "        {\"<br>\".join(split_info)}\n",
    "    </table>\n",
    "    \"\"\"\n",
    "    \n",
    "    # Display the output in the notebook\n",
    "    display(HTML(html_content))\n",
    "\n",
    "def get_available_device():\n",
    "    \"\"\"Helper method to find best possible hardware to run\n",
    "    Returns:\n",
    "\n",
    "    \"\"\"\n",
    "    # Check if CUDA is available\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\"), \"cuda\"\n",
    "    \n",
    "    # Check if ROCm is available\n",
    "    if torch.version.hip is not None and torch.backends.mps.is_available():\n",
    "        return torch.device(\"rocm\"), \"rocm\"\n",
    "    \n",
    "    # Check if MPS (Apple Silicon) is available\n",
    "    if torch.backends.mps.is_available():\n",
    "        return torch.device('cpu'), \"mps\"\n",
    "    \n",
    "    \n",
    "    # Fall back to CPU\n",
    "    return torch.device(\"cpu\"), \"cpu\"\n",
    "\n",
    "def get_installed_version(package_name):\n",
    "    with warnings.catch_warnings():\n",
    "        # Supress warnings from packages that have missing attributes that metadata will complain about.\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        distribution = metadata.Distribution()\n",
    "        try:\n",
    "            return distribution.from_name(package_name).version\n",
    "        except metadata.PackageNotFoundError:\n",
    "            return \"Not installed\"\n",
    "\n",
    "\n",
    "def display_configuration():\n",
    "    # Check device info\n",
    "    device, backend = get_available_device()\n",
    "\n",
    "    # Torch version\n",
    "    torch_version = torch.__version__\n",
    "\n",
    "    # HuggingFace Transformers version\n",
    "    transformers_ver = transformers.__version__\n",
    "\n",
    "    # BitsAndBytes version (if available)\n",
    "    bitsandbytes_version = get_installed_version(\"bitsandbytes\")\n",
    "\n",
    "    # Check for GPU-specific details if CUDA or ROCm is available\n",
    "    if device.type == \"cuda\":\n",
    "        cuda_device_count = torch.cuda.device_count()\n",
    "        cuda_device_name = torch.cuda.get_device_name(0)\n",
    "        cuda_version = torch.version.cuda\n",
    "    elif device.type == \"rocm\":\n",
    "        cuda_device_count = torch.cuda.device_count()\n",
    "        cuda_device_name = torch.cuda.get_device_name(0)\n",
    "        cuda_version = torch.version.hip\n",
    "    else:\n",
    "        cuda_device_count = 0\n",
    "        cuda_device_name = \"N/A\"\n",
    "        cuda_version = \"N/A\"\n",
    "\n",
    "    # Prepare HTML formatted output for better display in a notebook\n",
    "    html_content = f\"\"\"\n",
    "    <h2>System Configuration</h2>\n",
    "    <table>\n",
    "        <tr><td><strong>PyTorch version:</strong></td><td>{torch_version}</td></tr>\n",
    "        <tr><td><strong>Device:</strong></td><td>{device} (Backend: {backend})</td></tr>\n",
    "        <tr><td><strong>CUDA/ROCm version:</strong></td><td>{cuda_version}</td></tr>\n",
    "        <tr><td><strong>GPU count:</strong></td><td>{cuda_device_count}</td></tr>\n",
    "        <tr><td><strong>GPU name:</strong></td><td>{cuda_device_name}</td></tr>\n",
    "        <tr><td><strong>Hugging Face Transformers version:</strong></td><td>{transformers_ver}</td></tr>\n",
    "        <tr><td><strong>BitsAndBytes version:</strong></td><td>{bitsandbytes_version}</td></tr>\n",
    "    </table>\n",
    "    \"\"\"\n",
    "    \n",
    "    # Display the output in the notebook\n",
    "    display(HTML(html_content))\n",
    "\n",
    "\n",
    "def label_mapper(label: int) -> str:\n",
    "    \"\"\"Map label from int to string!\"\"\"\n",
    "    return ['Negative', \"Positive\"][label]\n",
    "\n",
    "def simple_truncate_text(row, max_length=50, tokenizer: transformers.PreTrainedTokenizerFast = None):\n",
    "    \"\"\"Example of a simple truncation method text, based on token count.\n",
    "    \n",
    "    You might want to perform 'smarter' truncation / summarization as a level, instead of simply cutting of after `max_length` tokens.\n",
    "    \n",
    "    Examples:\n",
    "        You might want to partially-apply the function, to provide a different tokenizer:\n",
    "        ```python3\n",
    "        from functools import partial\n",
    "        some_other_tokenizer = transformers.AutoTokenizer.from_pretrained('your_fave_tokenizer')\n",
    "        partial_simple_truncate = partial(simple_truncate_text, tokenizer=some_other_tokenizer)\n",
    "        ```\n",
    "    Args:\n",
    "        row (datasets....): Single instance or row of dataset.\n",
    "    \n",
    "    Keyword Args:\n",
    "        max_length (int, 150): the maximum length of text to be processed. Defaults to 150.\n",
    "        tokenizer (transformers.PreTrainedTokenizer, `fast_tokenizer`): the tokenizer to use. Defaults to `fast_tokenizer`.\n",
    "    \n",
    "    Notes:\n",
    "        This function requires all cells above to be run.\n",
    "    \"\"\"\n",
    "    token_representation = tokenizer.batch_encode_plus(row['text'], max_length=max_length, truncation=True)['input_ids']\n",
    "    text_representation = tokenizer.batch_decode(token_representation, skip_special_tokens=True)\n",
    "    row['text'] = text_representation\n",
    "    return row\n",
    "\n",
    "def generate(\n",
    "        input_text: str,\n",
    "        tokenizer: transformers.PreTrainedTokenizer,\n",
    "        model: transformers.PreTrainedModel,\n",
    "        generation_config: transformers.GenerationConfig,\n",
    ") -> str:\n",
    "    \"\"\"Helper method to generate a sample from the model, pre-conditioned on the input-text 'Prompt'.\n",
    "    \n",
    "    Args:\n",
    "        input_text (str): Input text to be conditioned on.\n",
    "        tokenizer (transformers.PreTrainedTokenizer): Tokenizer corresponding to the model provided.\n",
    "        model (transformers.PreTrainedModel): Pre-trained model to perform text generation with.\n",
    "        \n",
    "    Returns:\n",
    "        Generate text by the pre-conditioned model.\n",
    "    \"\"\"\n",
    "    \n",
    "    input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(model.device)\n",
    "    outputs = model.generate(input_ids=input_ids, generation_config=generation_config)\n",
    "    result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fec69498a66f823",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve 'best' backend and device\n",
    "device, backend = get_available_device()\n",
    "\n",
    "# Default bfloat16, because there is a lot of optimization\n",
    "dtype = torch.bfloat16\n",
    "\n",
    "# Optional bits-and-bytes configuration for additional quantization.\n",
    "bab_conf = None\n",
    "if backend == 'cuda':\n",
    "    # If you want, you can further quantize on CUDA devices (linux and WSL)\n",
    "    # However, this is more for you to explore than anything else.\n",
    "    bab_conf =  transformers.BitsAndBytesConfig(\n",
    "        load_in_8bit=False\n",
    "    )\n",
    "\n",
    "# Call the display_configuration() function in your Jupyter notebook to show the configuration\n",
    "display_configuration()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fde36c45670e1e",
   "metadata": {},
   "source": [
    "# Part 1: Preparing all the things\n",
    "\n",
    "Before we get started with our small lab experiment, we need to make sure that everything is prepared. Let's get started with setting up a small language model, and and loading and preparing the data.\n",
    "\n",
    "Recall from the lecture that this consists of the following 'recipe'.\n",
    "\n",
    "1. Load the model and data.\n",
    "   1. Load pre-trained or fine-tuned model\n",
    "   2. Load dataset and tokenize\n",
    "2. Run the data through the model\n",
    "3. Perform experiments (+ Analysis)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b63a69f16551143",
   "metadata": {},
   "source": [
    "## Step 1: Preparing The Model\n",
    "Loading the model and see that it work, we will use the Flan-T5 model by Google / DeepMind. This model is tiny and should be fast enough even on lower powered hardware!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb66d1eca874f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tokenizer for flan family\n",
    "family: str = \"google/flan-t5\"\n",
    "\n",
    "# For the Lab we will use a small model, just to provide some insight into usability.\n",
    "model: str = f\"-small\" # # '-base', '-large'\n",
    "\n",
    "model_name: str = f\"{family}{model}\"\n",
    "# Create tokenizer\n",
    "# import os\n",
    "# HF_TOKEN='hf_...'\n",
    "tokenizer: T5Tokenizer = T5Tokenizer.from_pretrained(model_name, legacy=False)\n",
    "dtype = torch.bfloat16 # torch.float16\n",
    "# Instantate model and load to the correct device.\n",
    "model: T5ForConditionalGeneration = T5ForConditionalGeneration.from_pretrained(\n",
    "    pretrained_model_name_or_path=model_name,\n",
    "    device_map=device,\n",
    "    torch_dtype=dtype,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44cf2cd6230b091b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we check that everything is working, note that the story should be quite bad, as T5 is not really trained to tell us stories.\n",
    "\n",
    "input_text = \"Write a story about a dog and a boy playing with a ball on a boat with sailors.\"\n",
    "# N.B., this might differ slighty for different versions of libraries.\n",
    "expected_response = 'The dog and the boy are playing with a ball on a boat. They are chasing each other in the water. The dog is chasing the other dog. The dog is chasing the other dog. The dog is chasing'\n",
    "\n",
    "deterministic_config = transformers.GenerationConfig(do_sample=False, max_length=50, min_length=25, repetition_penalty=1.19)\n",
    "deterministic_reponse = generate(\n",
    "            input_text=input_text,\n",
    "            tokenizer=tokenizer,\n",
    "            model=model,\n",
    "            generation_config=deterministic_config\n",
    "        )\n",
    "\n",
    "do_sample_config = transformers.GenerationConfig(do_sample=True, max_length=50, min_length=25, repetition_penalty=1.19)\n",
    "random_response = generate(\n",
    "            input_text=input_text,\n",
    "            tokenizer=tokenizer,\n",
    "            model=model,\n",
    "            generation_config=do_sample_config\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75133491d38afc8c",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "display(\n",
    "    Markdown(\"### Generated text by the LLM\"),\n",
    "    Markdown(f\"> {input_text}\"),\n",
    "    Markdown('*Deterministic Generation*'),\n",
    "    Markdown(\n",
    "        textwrap.dedent(\n",
    "            f\"\"\"\\\n",
    "            | Deterministic           | Deterministic (response) | Equal (True/False)                            |\n",
    "            |-------------------------|--------------------------|-----------------------------------------------|\n",
    "            | {expected_response}     | {deterministic_reponse}  | {expected_response == deterministic_reponse}  |\n",
    "            \"\"\"\n",
    "        )\n",
    "    ),\n",
    "    Markdown('*Random Generation*'),\n",
    "    Markdown(\n",
    "        textwrap.dedent(\n",
    "            f\"\"\"\\\n",
    "            | Deterministic           | Random (response)        | Equal (True/False)                           |\n",
    "            |-------------------------|--------------------------|----------------------------------------------|\n",
    "            | {expected_response}     | {random_response}        |{random_response == deterministic_reponse}    |\n",
    "            \"\"\"\n",
    "        )\n",
    "    )    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d787247f9559eb",
   "metadata": {},
   "source": [
    "## Step 1.5 Preparing the Data.\n",
    "\n",
    "As we will be working with a Semtiment 'classification' task, as the only labels are `Postive` (`1`) or `Negative` (`0`). First, we will need to load the appropriate dataset (`standfordnlp/imbd`), which contains movie reviews and their respective label. During the rest of the lab, we will further investigate how to do pre-processing of the data, run (different types of) inference, and perform fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a727f6b877435832",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dataset name\n",
    "data_name: str = 'stanfordnlp/imdb'\n",
    "\n",
    "# Load dataset, and assign splits to variables\n",
    "dataset: datasets.DatasetDict = datasets.load_dataset(data_name)\n",
    "train_set: datasets.Dataset = dataset['train']\n",
    "test_set: datasets.Dataset = dataset['test']\n",
    "# This unsupervised split is not used in the rest of the notebook.\n",
    "unsup: datasets.Dataset = dataset['unsupervised']\n",
    "\n",
    "# Give an overview\n",
    "display_dataset_description(data_name, dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765a3d7cad2c7b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sample = train_set[1231]\n",
    "review_1, label_1 = sample['text'], label_mapper(sample['label'])\n",
    "sample = train_set[15442]\n",
    "review_2, label_2 = sample['text'], label_mapper(sample['label'])\n",
    "\n",
    "\n",
    "display(\n",
    "    Markdown(\n",
    "        textwrap.dedent(f\"\"\"\\\n",
    "        | *Example*                 | Label     |\n",
    "        |:--------------------------|:---------:|\n",
    "        | {review_1}                | {label_1} |\n",
    "        | {review_2}                | {label_2} |\n",
    "        \"\"\")\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e889eb982bd5568",
   "metadata": {},
   "source": [
    "Next we will create some dataloader to ensure that we can quickly load data into the model, making the rest of the cells load a little faster.\n",
    "\n",
    "Let's also define some library functions, that we can use to calculate the performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52aba2cc1c8819d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First determine some hyper-parameters, this should be fine on even a small model and CPU only\n",
    "\n",
    "# If you have a GPU / powerful machine, feel free to increase the following\n",
    "batch_size = 1\n",
    "test_samples = 2000\n",
    "max_iterations = test_samples // batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a3b324d7a9e3b2",
   "metadata": {},
   "source": [
    "# Approach 1: Simple Prompting\n",
    "\n",
    "Rather than going straight into a complex solution, let's first see what we can achieve by letting the model predict the output.\n",
    "\n",
    "\n",
    "> Note, I annotate the 'steps' in comments. There might be code sections that we will fill in during the lab, annotated with.\n",
    "\n",
    "\n",
    "```python\n",
    "# YOUR CODE GOES HERE!\n",
    "# END OF YOUR CODE!\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdaae94fd14c7426",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "def simple_prompt_function(batch):\n",
    "    \"\"\"Simple prompt preparation function.\"\"\"\n",
    "    stringified_representation = list(map(lambda x: f\"Positive/Negative? {x})\", batch['text']))\n",
    "    batch['text'] = stringified_representation\n",
    "    return batch\n",
    "\n",
    "def simple_template_function(batch, template=None):\n",
    "    \"\"\"Mapping function using a template. Note, we will show in the lab to set this up.\"\"\"\n",
    "    stringified_representation = [template.render(review=review) for review in batch['text']]\n",
    "    batch['text'] = stringified_representation\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8ff65650b3f67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sub_sampled_set = test_set.shuffle(seed=123).take(test_samples)\n",
    "tokenized_eval_dataset = (\n",
    "    sub_sampled_set\n",
    "    .map(simple_prompt_function, batched=True)\n",
    "    .map(lambda batch: tokenizer(batch['text']))\n",
    ")\n",
    "tokenized_eval_dataset.set_format('torch', columns=['input_ids', 'label', 'attention_mask'])\n",
    "# TODO: Let's re-write to use a template!\n",
    "\n",
    "\n",
    "# YOUR CODE GOES HERE\n",
    "...\n",
    "# END OF YOUR CODE\n",
    "\n",
    "# Display the de-tokenized text\n",
    "display(\n",
    "    Markdown('### What the model `sees`'),\n",
    "    Markdown(\n",
    "        f\"\"\"{tokenized_eval_dataset[0]['input_ids'][:100]}...\"\"\"\n",
    "    ),\n",
    "    Markdown('### What we would `see`'),\n",
    "    Markdown(\n",
    "        f\"\"\"{tokenizer.decode(tokenized_eval_dataset[0]['input_ids'], skip_special_tokens=True)}\"\"\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5d47c35bbf7fd5",
   "metadata": {},
   "source": [
    "# Part 2: Running Inference\n",
    "\n",
    "Now that we got the setup out of way, we can start 'running experiments'. In short this boils down to performing 3 steps;\n",
    "\n",
    "1. Choosing your hyper-parameters and choosing appropriate levels\n",
    "2. Getting a script ready to run your experiments\n",
    "3. ** Run the experiments.** (Or, an excellent time to get coffee :P)\n",
    "\n",
    "This part of the lab will focus on that last point, to ensure that you can run your experiment efficiently, in the tutorial we are going to fix some issues with the code below, and make it run *fast*er (with some caveats)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a8a18142c5a73",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_fn(data_loader, evaluate_model=None) -> Tuple[List[List[int]], List[List[int]]]:\n",
    "    labels_list = []\n",
    "    prediction_list = []\n",
    "    for batch in tqdm(data_loader):\n",
    "        input_ids, attention_mask, label = batch['input_ids'].to(evaluate_model.device), batch['attention_mask'].to(evaluate_model.device), batch['label'].to(evaluate_model.device)\n",
    "        outputs = evaluate_model.generate(\n",
    "          input_ids=input_ids,\n",
    "          attention_mask=attention_mask,\n",
    "          max_length=3,\n",
    "          do_sample=False,\n",
    "        )\n",
    "        prediction = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "        prediction_list.append(prediction)\n",
    "        labels_list.append(label.tolist())\n",
    "    return prediction_list, labels_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84225092638034ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# YOUR CODE GOES HERE\n",
    "...\n",
    "# END OF YOUR CODE \n",
    "# 5. Set the format of the dataset to PyTorch Tensors\n",
    "eval_loader = torch.utils.data.DataLoader(\n",
    "    tokenized_eval_dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "\n",
    "# And run the evaluation\n",
    "predictions_list, labels_list = evaluate_fn(eval_loader, evaluate_model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edcd2c5784bac4f6",
   "metadata": {},
   "source": [
    "### Retrieving the Results\n",
    "\n",
    "Lastly, we will inspect the results of this 'experiment'. Think about some of the caveats, and how to addres them in code (don't worry, the HW does not have (all) caveats), but it is good to be aware of them!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59d0cdcc31ab4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_evaluation(Y_hat, Y):\n",
    "    flat_predictions = list(itertools.chain(*Y_hat))\n",
    "    flat_labels = list(itertools.chain(*Y))\n",
    "    \n",
    "    label_lut = defaultdict(lambda: -1, {'positive': 1, 'negative': 0})\n",
    "    predictions = list(map( lambda x: label_lut[x.split(' ')[0].lower()], flat_predictions))\n",
    "    \n",
    "    accuracy = sum(map(lambda x: x[0] == x[1], zip(predictions, flat_labels))) / len(flat_labels)\n",
    "    unknown = sum(map(lambda x: x[0] == -1, zip(predictions, flat_labels))) / len(flat_labels)\n",
    "    \n",
    "    return accuracy, unknown\n",
    "\n",
    "\n",
    "accuracy, unknown = get_evaluation(predictions_list, labels_list)\n",
    "display(\n",
    "    Markdown(\n",
    "        textwrap.dedent(\n",
    "            f\"\"\"\\\n",
    "            | *Accuracy*  | *Unknown*      |\n",
    "            |:------------|:---------------|\n",
    "            | {accuracy}  | {unknown}      |\n",
    "            \"\"\")\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c538e9b9ec66d1c4",
   "metadata": {},
   "source": [
    "# Part 2: Fine-Tuning the Model\n",
    "\n",
    "Next, we will show the basics of performing fine-tuning of the model, herein, we are going to investigate\n",
    "\n",
    "1. Loading the model\n",
    "2. Defining Hyper-Parameters corresponding to the training process\n",
    "3. Perform evaluation with the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d4854c7cca9ce",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "import peft\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from typing import Tuple\n",
    "\n",
    "\n",
    "def get_peft_details(model):\n",
    "    trainable_model_params = 0\n",
    "    all_model_params = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_model_params += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_model_params += param.numel()\n",
    "    return  trainable_model_params, all_model_params, (100 * trainable_model_params) / all_model_params\n",
    "\n",
    "\n",
    "\n",
    "def train_model(\n",
    "        peft_model,\n",
    "        peft_training_args,\n",
    "        train_set,\n",
    "        test_set = None,\n",
    ") -> Tuple[transformers.Trainer, peft.PeftModel]:\n",
    "    peft_trainer = transformers.Trainer(\n",
    "        model=peft_model,\n",
    "        args=peft_training_args,\n",
    "        train_dataset=train_set,\n",
    "        eval_dataset=test_set,\n",
    "    )\n",
    "    # Pre-train the model\n",
    "    peft_trainer.train()\n",
    "    return peft_model, peft_trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7beae1ca02849daa",
   "metadata": {},
   "source": [
    "## Step 1: Prepare Data and Model\n",
    "\n",
    "Let's continue by preparing a model, note that a lot of this is 'boiler-plate', to reduce the trainign time considerably.\n",
    "\n",
    "If you are interested, and/or want to apply this to your project, we recommend looking into (HF tutorials of) the following:\n",
    "\n",
    "1. Quantization, where and how to apply it. For fine-tuning, we use this in the `LoraConfig`, or Low-Rank Adaptation config, which approximates the full-rank of the gradient with a lower-rank decomposition, thereby considerably reducign the overehad brought by the back-propagation\n",
    "2. Data-types, and when to use them; besides working well for training, inference may also benefit from quantization. In general, experiments are run in 'half' precision (`torch.float16` or `torch.bfloat16`), but lower preicsion exists as well (as low as 1 bit (XOR-quantization))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e156cc1a514e18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # In case we have already defined the peft-model, we remove it.\n",
    "    del peft_model, model\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    model = T5ForConditionalGeneration.from_pretrained(\n",
    "    pretrained_model_name_or_path=model_name,\n",
    "    device_map=device,\n",
    "    torch_dtype=dtype,\n",
    "    )\n",
    "except:\n",
    "    pass\n",
    "# Example of hyper-parameters.\n",
    "RANK = 16               # Rank used in model update (lower is faster, less precise)\n",
    "ALPHA = 32\n",
    "# Scaling factor for update (∆W x dy ALPHA/RANK)           \n",
    "DROPOUT = 0.05          # Regularization term\n",
    "TRAIN_BATCH_SIZE = 32   # Number of samples\n",
    "TRAIN_EPOCHS = 5        # Total number of training steps.\n",
    "LR=5e-4\n",
    "# If you want to save some time, you can store checkpoints, and load them, to create multiple levels\n",
    "# in a single run. Do note, that huggingface by default uses learning-rate scheduling, so this may\n",
    "# affect your results a bit.\n",
    "\n",
    "# The modules are specific to the model itself.\n",
    "MODULES = None # ['k', 'v'] # 'or any other identifier of weights.\n",
    "TORCH_DTYPE = torch.bfloat16\n",
    "\n",
    "# TODO: Decide the levels for your experiment. These can be any of the \n",
    "# aforementioned parameters, or any other hyper-parameter.\n",
    "lora_config = LoraConfig(\n",
    "    r=RANK,\n",
    "    lora_alpha=ALPHA,\n",
    "    target_modules=MODULES,\n",
    "    lora_dropout=DROPOUT,\n",
    "    bias='none',\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM\n",
    ")\n",
    "peft_model = get_peft_model(\n",
    "    model=model,\n",
    "    peft_config=lora_config,\n",
    ")\n",
    "\n",
    "# Define training parameters\n",
    "output_dir = 'llm_lab/t5-small'\n",
    "\n",
    "train_config = transformers.TrainingArguments(\n",
    "    output_dir='./tutorial-3',\n",
    "    per_device_train_batch_size=100,\n",
    "    do_train=True,\n",
    "    do_eval=False,\n",
    "    learning_rate=LR,\n",
    "    # num_train_epochs=TRAIN_EPOCHS,\n",
    "    num_train_epochs=1,\n",
    "    max_steps=100,\n",
    "    save_strategy='steps',\n",
    "    save_steps=500,\n",
    "    logging_strategy='steps',\n",
    "    logging_steps=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86a347b69f67397",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "pft, orig, pct = get_peft_details(peft_model)\n",
    "display(\n",
    "    Markdown(\n",
    "        textwrap.dedent(\n",
    "            f\"\"\"\\\n",
    "            | Parameter        | Statistic |\n",
    "            |:-----------------|:----------|\n",
    "            | Original         | {orig}    |\n",
    "            | PEFT             | {pft}     |\n",
    "            | Percentage       | {pct:.2f}%|\n",
    "            \"\"\"\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2e5beba93ce2e0",
   "metadata": {},
   "source": [
    "## Step 2: And Lift-off (ish)\n",
    "Let's do a round of training, and look at'er go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf06fc9e9eb9285",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some helper functions\n",
    "def limit_by_tokens(batch, max_length: int = 150, tokenizer: transformers.AutoTokenizer = tokenizer):\n",
    "    \"\"\"Helper method to limit the string length using the user defined tokenizer\n",
    "    \n",
    "    Args:\n",
    "        batch (LazyBatch): Batch to tokenizer.\n",
    "    \n",
    "    Keyword Args:\n",
    "        max_length (int): maximum lenght to tokenize data to, defaults to 150.\n",
    "        tokenizer (transformers.AutoTokenizer, optional): Tokenizer of the model to use.\n",
    "        \n",
    "    Returns:\n",
    "        LazyBatch: Batch with limited string length in string fromat.\n",
    "    \"\"\"\n",
    "    token_repr = tokenizer.batch_encode_plus(batch['text'], max_length=max_length, padding=True, truncation=True, add_special_tokens=False)['input_ids']\n",
    "    short_string = tokenizer.batch_decode(token_repr, skip_special_tokens=True)\n",
    "    batch['text'] = short_string\n",
    "    return batch\n",
    "\n",
    "def tokenize_function(\n",
    "        batch,\n",
    "        prefix='Is the following Positive or Negative?\\n',\n",
    "        post_fix='\\nAnswer: '):\n",
    "\n",
    "    updated_text = [f\"{prefix}{review}{post_fix}\" for review in batch[\"text\"]]\n",
    "    batch['text'] = updated_text\n",
    "    tokenized_text = tokenizer.batch_encode_plus(updated_text, padding=True)\n",
    "    batch['input_ids'] = tokenized_text.input_ids\n",
    "    batch['attention_mask'] = tokenized_text.attention_mask\n",
    "    # We also set the 'response', i.e., what the model should learn\n",
    "    batch['labels'] = tokenizer.batch_encode_plus(['Positive' if label == 1 else 'Negative' for label in batch[\"label\"]], padding='max_length', truncation=True, max_length=3, return_tensors=\"pt\").input_ids\n",
    "\n",
    "    return batch\n",
    "\n",
    "# If the number of tokens is a level, you might need to change this\n",
    "train_ready_set = (\n",
    "    train_set\n",
    "    .map(limit_by_tokens, batched=True, num_proc=10, batch_size=200)\n",
    "    .map(tokenize_function, batched=True)\n",
    ")\n",
    "example = train_ready_set[0]\n",
    "newline = '\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9a6dbe4a855557",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample\n",
    "display(\n",
    "    Markdown(\n",
    "        textwrap.dedent(\n",
    "            f\"\"\"\\\n",
    "            | *Key*             | *Example*              |\n",
    "            |-------------------|------------------------|\n",
    "            | *text*            | {example['text'].replace(newline, '<br>')}       |\n",
    "            | *input_ids*      | {example['input_ids']}       |\n",
    "            | *labels*         | {example['labels']}       |\n",
    "            | *detokenized_labels*| {tokenizer.decode(example['labels'])}   |\n",
    "            \"\"\"\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41471c6eb0dbd085",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate with original model.\n",
    "\n",
    "test_dataset = (\n",
    "    test_set\n",
    "    .map(limit_by_tokens, batched=True, num_proc=10, batch_size=200)\n",
    "    .map(\n",
    "        tokenize_function, batched=True\n",
    "    )\n",
    "    .map(\n",
    "        lambda batch: tokenizer.batch_encode_plus(\n",
    "            batch['text'],\n",
    "            add_special_tokens=True,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=False,\n",
    "        ), batched=True\n",
    "    )\n",
    ")\n",
    "# Ensure we can effectively use the model\n",
    "test_dataset.set_format(type='torch', columns=['input_ids', 'label', 'attention_mask'])\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "    dataset=test_dataset, # .shuffle(seed=123).take(2000),\n",
    "    batch_size=400,  # Feel free to lower / higher this\n",
    "    shuffle=False,  # Shuffling not needed during evaluation\n",
    "    num_workers=5,  # Feel free to set this to -1 or 1, esp. on CPU \n",
    "    prefetch_factor=200,\n",
    ")\n",
    "\n",
    "with torch.no_grad():\n",
    "    result, labels = evaluate_fn(test_dataloader, evaluate_model=model)\n",
    "accuracy, unknown = get_evaluation(result, labels)\n",
    "display(\n",
    "    Markdown(\n",
    "        textwrap.dedent(\n",
    "            f\"\"\"\\\n",
    "            | *Accuracy*  | *Unknown*      |\n",
    "            |:------------|:---------------|\n",
    "            | {accuracy}  | {unknown}      |\n",
    "            \"\"\")\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4179dc6b36b4e5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "peft_model, peft_trainer = train_model(\n",
    "    peft_model=peft_model,\n",
    "    peft_training_args=train_config,\n",
    "    train_set=train_ready_set,\n",
    "    test_set=None,\n",
    ")\n",
    "peft_model.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db091d66fe793423",
   "metadata": {},
   "source": [
    "### Step 2.1 Let's evaluate the model...\n",
    "\n",
    "Can you think fo some caveats fo the model, what would happen if:\n",
    "\n",
    "1. We change the prompt after training?\n",
    "2. We change the input length of the model?\n",
    "3. We want to include additional sentiments, such as `neutral`?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b259008d1cafa46",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with torch.no_grad():\n",
    "    peft_model.eval()\n",
    "    result, labels = evaluate_fn(test_dataloader, evaluate_model=peft_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbc9b48d551094b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "accuracy, unknown = get_evaluation(result, labels)\n",
    "display(\n",
    "    Markdown(\n",
    "        textwrap.dedent(\n",
    "            f\"\"\"\\\n",
    "            ### Results after PEFT fine-tuning\n",
    "            | *Accuracy*  | *Unknown*      |\n",
    "            |:------------|:---------------|\n",
    "            | {accuracy}  | {unknown}      |\n",
    "            \"\"\"\n",
    "        )\n",
    "    ),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94e57ef3941610a",
   "metadata": {},
   "source": [
    "# Part 3: Few-Shot inference\n",
    "\n",
    "During the last session we will cover running few-shot learning, a derivative from the 'Zero-Shot' prompting, that we have seen before.\n",
    "\n",
    "In contrast to zero-shot learning, we provide some 'examples' to the model, consisting of a 'Review' and a classification of 'Sentiment' (Positive/Negative). Consecutively, we prompt the model to classify the actual piece of information that we are interested in. As such, this consists of the following parts;\n",
    "\n",
    "1. Selecting *which* examples to use,\n",
    "2. Selecting *how to represent the examples,\n",
    "3. Pre-processing the data (similar to what we've seen before).\n",
    "4. Do inference.\n",
    "\n",
    "\n",
    "Generally, in few-shot, we can control the number of examples, i.e., shots, with a parameter $k$. So a k-shot example. Throughout this example, we will focus on a $k$ with 2 Positive and 2 Negative examples. However, in your experiments, you may wish to control $k$, or even de split of Positive and Negative examples that you will provide to the model.\n",
    "\n",
    "> The goal of this part is to show how you can create a flexible template, apply it to the data, and evaluate few-shot experiments effectively.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc4d8ac03aadb5",
   "metadata": {},
   "source": [
    "## Step 1: Selecting Few-Shot Examples\n",
    "\n",
    "First, we will need to select a few simple examples, for this we need to think about to sample from where, and to make sure that we can select relevant samples.\n",
    "\n",
    "Let us first define some helper functions to use durin gthis part of the lab.\n",
    "\n",
    "**N.B.**, some of these functions are redundant, but that is to prevent to have to 'cherry pick' the cells from above that\n",
    "we want to run. Alternatively, you may want to choose to write a small library to import helper functions from a single,\n",
    "file, but this is out of the scope of this course.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f3e2dada53315b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some helper functions\n",
    "\n",
    "def sentiment_split_dataset(dataset: datasets.Dataset):\n",
    "    \"\"\"Helper function to split a dataset into a positive and negative set using their provided label.\n",
    "    \n",
    "    Args:\n",
    "        dataset (dataset.Dataset): Dataset (split) to seperate into positive and negative datasets.\n",
    "    \"\"\"\n",
    "    \n",
    "    positive_split = dataset.filter(lambda sample: sample['label'] == 1)\n",
    "    negative_split = dataset.filter(lambda sample: sample['label'] == 0)\n",
    "    \n",
    "    return positive_split, negative_split\n",
    "\n",
    "def limit_by_tokens(batch, max_length: int = 150, tokenizer: transformers.AutoTokenizer = None):\n",
    "    \"\"\"Helper method to limit the string length using the user defined tokenizer\n",
    "    \n",
    "    Args:\n",
    "        batch (LazyBatch): Batch to tokenizer.\n",
    "    \n",
    "    Keyword Args:\n",
    "        max_length (int): maximum lenght to tokenize data to, defaults to 150.\n",
    "        tokenizer (transformers.AutoTokenizer, optional): Tokenizer of the model to use.\n",
    "        \n",
    "    Returns:\n",
    "        LazyBatch: Batch with limited string length in string fromat.\n",
    "    \"\"\"\n",
    "    token_repr = tokenizer.batch_encode_plus(batch['text'], max_length=max_length, padding=True, truncation=True, add_special_tokens=False)['input_ids']\n",
    "    short_string = tokenizer.batch_decode(token_repr, skip_special_tokens=True)\n",
    "    batch['text'] = short_string\n",
    "    return batch\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_fn(data_loader, evaluate_model) -> Tuple[List[List[str]], List[List[int]]]:\n",
    "    \"\"\"Evaluation of dataloader and model, note that deterministic (`do_sampling=False`) is used, as we ask the model\n",
    "    to classify positive and negative examples.\n",
    "    \n",
    "    Args:\n",
    "        data_loader (DataLoader): Dataloader containing evaluation data.\n",
    "        evaluate_model (transformers.AutoModelForSeq2SeqLM): Model used for evaluation.\n",
    "        \n",
    "    Returns:\n",
    "        List of lists of strings containing the model's prediction.\n",
    "        List of lists of integers representing the ground truth labels.\n",
    "    \"\"\"\n",
    "    labels_list = []\n",
    "    prediction_list = []\n",
    "    for batch in tqdm(data_loader):\n",
    "        input_ids, attention_mask, label = batch['input_ids'].to(evaluate_model.device), batch['attention_mask'].to(evaluate_model.device), batch['label'].to(evaluate_model.device)\n",
    "        outputs = evaluate_model.generate(\n",
    "          input_ids=input_ids,\n",
    "          attention_mask=attention_mask,\n",
    "          max_length=3,\n",
    "          do_sample=False,\n",
    "        )\n",
    "        prediction = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "        prediction_list.append(prediction)\n",
    "        labels_list.append(label.tolist())\n",
    "    return prediction_list, labels_list\n",
    "\n",
    "\n",
    "def get_evaluation(Y_hat: List[List[str]], Y: List[List[int]]):\n",
    "    \"\"\"Helper function to perform evaluation.\n",
    "    \n",
    "    Args:\n",
    "        Y_hat (List[List[str]]): Minibatched prediction results (str) of a language model.\n",
    "        Y (List[List[int]]): Minibatched ground-truth labels (int) of a dataset.\n",
    "\n",
    "    Returns:\n",
    "        float representing the accuracy of the model.\n",
    "        float representing the fraction of data for which neither positive or negative was given.\n",
    "    \"\"\"\n",
    "    flat_predictions = list(itertools.chain(*Y_hat))\n",
    "    flat_labels = list(itertools.chain(*Y))\n",
    "\n",
    "    label_lut = defaultdict(lambda: -1, {'positive': 1, 'negative': 0})\n",
    "    predictions = list(map(lambda x: label_lut[x.split(' ')[0].lower()], flat_predictions))\n",
    "\n",
    "    accuracy = sum(map(lambda x: x[0] == x[1], zip(predictions, flat_labels))) / len(flat_labels)\n",
    "    unknown = sum(map(lambda x: x[0] == -1, zip(predictions, flat_labels))) / len(flat_labels)\n",
    "\n",
    "    return accuracy, unknown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5021a0d0d84e60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "positive_dataset, negative_dataset = sentiment_split_dataset(train_set)\n",
    "k_shots = 4\n",
    "shots = []\n",
    "labels = []\n",
    "# TODO: select $k$ samples, and create a list of 'shots' and 'labels'\n",
    "# YOUR CODE GOES HERE\n",
    "...    \n",
    "# END OF YOUR CODE\n",
    "\n",
    "# Here, we combine the labels and shots to get a single result\n",
    "few_shots = list(zip(shots, labels))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48db798e032558a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper definition to render the results (truncated)...\n",
    "newline = '\\n'\n",
    "display(\n",
    "    Markdown('### Selected shots: '),\n",
    "    Markdown(textwrap.dedent(\n",
    "        f\"\"\"\\\n",
    "        | Shot          |       Review          | Sentiment     |\n",
    "        |---------------|-----------------------|---------------|\n",
    "        {\"        \".join(f\"| {idx} | {shot[:100].replace(newline, '<br>')}... | {label} |{newline}\" for idx, (shot, label) in enumerate(few_shots))}\"\"\")\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e57b1e5b11cc04c",
   "metadata": {},
   "source": [
    "## Step 2: Preparing the Few-Shot Prompt\n",
    "\n",
    "Next, we need to design a simple approach to make sure we can represent the data to the model, effectively, we want to be able to provide the model with something like the following:\n",
    "\n",
    "\n",
    "> Context:\n",
    ">  Review: I liked the dramatic opening of the first scene.\\\n",
    ">  Sentiment: Positive\n",
    ">\n",
    ">  Review: That actor makes me want to throw my phone away.\\\n",
    ">  Sentiment: Negative\n",
    ">    \n",
    ">  Question: Is the following positive or negative?\\\n",
    ">  Review: I like turtles.\\\n",
    ">  Sentiment:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5fbf3cca0df23d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jinja2\n",
    "import textwrap\n",
    "\n",
    "template = jinja2.Template(\n",
    "    textwrap.dedent(\n",
    "        ### YOUR CODE GOES HERE\n",
    "        ...\n",
    "        ### END OF YOUR CODE\n",
    "    )\n",
    ")\n",
    "\n",
    "example_shots: List[Tuple[str]] = [('Well, hello there.', 'Positive'), ('I like turtles', 'Positive'), (\"...\", 'Negative'), ('Whelp, I cannot think of other small example reviews', 'Negative')]\n",
    "\n",
    "review: str = 'De kat krabt de krullen van de trap.'\n",
    "example_prompt: str = template.render(shots=example_shots, review=review).replace('\\n', '<br>')\n",
    "\n",
    "display(\n",
    "    Markdown(\"### Example of your rendered template\"),\n",
    "    Markdown(f\"> {example_prompt}\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cec0b7a6a88c3c6",
   "metadata": {},
   "source": [
    "## Step 3: Preparing the data\n",
    "\n",
    "Next, we will pre-process the data as we have seen before, here we ensure:\n",
    "\n",
    "1. We render the input using the `Template` we just designed.\n",
    "2. We provide the appropriate examples that we selected earlier.\n",
    "3. We tokenize the data.\n",
    "\n",
    "To speed up this part, we have collated these steps in a single function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b93ab84844d048",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def process_few_shot_batch(batch, template: jinja2.Template, shots: List[Tuple[str, str]], tokenizer = None, randomize=False):\n",
    "    \"\"\"Simple all-in-one render and tokenization method to render few-shots examples.\n",
    "    \n",
    "    Args:\n",
    "        batch (LazyBatch): batch to render as a few-shot sample.\n",
    "        template (Template): Jinja template to use to render few-shot examples.\n",
    "        shots (List[Tuple[str, str]]): List of shots to render.\n",
    "        tokenizer (Tokenizer): Model tokenizer.    \n",
    "    \"\"\"\n",
    "    rendered_samples = [template.render(review=sample, shots=random.sample(shots, k=len(shots))) for sample in batch['text']]\n",
    "    if tokenizer is not None:\n",
    "        return tokenizer.batch_encode_plus(rendered_samples, padding=True, add_special_tokens=True, truncation=False, return_tensors='pt')\n",
    "    else:\n",
    "        batch['text'] = rendered_samples\n",
    "        return batch\n",
    "\n",
    "few_shot_evaluation = (\n",
    "    test_set\n",
    "    .shuffle(seed=421)\n",
    "    .take(2000)\n",
    "    .map(limit_by_tokens, batched=True, num_proc=10, batch_size=200)\n",
    "    .map(lambda batch: process_few_shot_batch(batch, template, few_shots, tokenizer), batched=True)\n",
    ")\n",
    "# And set the dataset format to something expected. :)\n",
    "few_shot_evaluation.set_format(type='torch', columns=['input_ids', 'label', 'attention_mask'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945077151632c272",
   "metadata": {},
   "source": [
    "## Step 4: Running inference.\n",
    "\n",
    "Next, we run the inference on the model, to see how the model improves over the prior art."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6b8017300111c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate with original model.\n",
    "\n",
    "# N.B., We set the batch size lower, as we increase the lenght of the input prompt.\n",
    "few_shot_loader = torch.utils.data.DataLoader(\n",
    "    dataset=few_shot_evaluation, # .shuffle(seed=123).take(2000),\n",
    "    batch_size=50,  # Feel free to lower / higher this\n",
    "    shuffle=False,  # Shuffling not needed during evaluation\n",
    "    num_workers=2,  # Feel free to set this to -1 or 1, esp. on CPU \n",
    "    prefetch_factor=50,\n",
    ")\n",
    "\n",
    "with torch.no_grad():\n",
    "    result, labels = evaluate_fn(few_shot_loader, evaluate_model=model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8abea33cd73c54ba",
   "metadata": {},
   "source": [
    "## Step 5: Looking at the results\n",
    "\n",
    "Lastly, we evaluate the results, can you think of ways to further improve the results? What would happen if we:\n",
    "\n",
    "1. Would take different few-shot examples?\n",
    "2. Would take more few-shot examples?\n",
    "3. Take different numbers of Positive/Negative samples?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf82bba09eb8601",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy, unknown = get_evaluation(result, labels)\n",
    "display(\n",
    "    Markdown(\n",
    "        textwrap.dedent(\n",
    "            f\"\"\"\\\n",
    "            | *Accuracy*  | *Unknown*      |\n",
    "            |:------------|:---------------|\n",
    "            | {accuracy}  | {unknown}      |\n",
    "            \"\"\")\n",
    "    ),\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
