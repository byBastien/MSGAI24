{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "497b74ed0b990037",
   "metadata": {},
   "source": [
    "# Lab on Prompting and fine-tuning\n",
    "\n",
    "In this lab, we will cover the basics that are needed to perform the homework 2 on Prompting and Fine-Tuning LLMs. As you will see in the next section, you will see that the order of the lab is sligthly different from the HW. This is intentional, as the fine-tuning of a model will take additional time, whereas the Few-Shot exercise (2) builds on the knowledge of exercise 1.\n",
    "\n",
    "## Layout of the Lab\n",
    "\n",
    "We will cover the following in the first part;\n",
    "\n",
    "1. Preparation of the `model` and `dataset`,\n",
    "2. Pre-processing of the `dataset`, using `jinja2` templating,\n",
    "3. Running inference experiments.\n",
    "\n",
    "In the second part, we will focus on;\n",
    "\n",
    "1. Fine-Tuning a model,\n",
    "2. Hyper-parameters to consider,\n",
    "3. Running inference with a fine-tuned model.\n",
    "\n",
    "In the third part, which will likely be mostly the 2nd lab (next week), focusses on ;\n",
    "1. Few-shot Learning,\n",
    "2. Creating context,\n",
    "3. Running inference experiemnts.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d16bf6f9c4752f",
   "metadata": {},
   "source": [
    "## Installing dependencies\n",
    "Just to be sure, run the following to install (any missing) dependencies. You only have to run this once if you have persistent storage for you `venv` or `conda` environment. However, jsut to be shure, you might want to run this again. \n",
    "\n",
    "> Depending on your bandwith, disk, and CPU this might take a while.\n",
    "\n",
    "## Change log\n",
    "* 23 Oct 2024: we have updated the dependencies to include `protobuf` needed to load certain models.\n",
    "* 23 Oct 2024: we have updated the Part 1 & 2 to address minor issues in the code.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "41ffb0dfeb61e459",
   "metadata": {
    "tags": [
     "hide-cell"
    ],
    "ExecuteTime": {
     "end_time": "2024-10-30T16:32:46.185021Z",
     "start_time": "2024-10-30T16:32:45.476690Z"
    }
   },
   "source": [
    "# Install the packages, if you run this a second time with persistent storage, feel free to skip.\n",
    "%pip install numpy~=1.26.0 torch~=2.2.1 transformers accelerate datasets bitsandbytes sentencepiece peft accelerate nbconvert==6.5.4 pypdf2==2 \"lxml[html_clean]\" notebook-as-pdf seaborn protobuf"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy~=1.26.0 in /Users/bastienjossen/anaconda3/envs/pytorch_mps/lib/python3.11/site-packages (1.26.4)\r\n",
      "Requirement already satisfied: torch~=2.2.1 in /Users/bastienjossen/anaconda3/envs/pytorch_mps/lib/python3.11/site-packages (2.2.2)\r\n",
      "Requirement already satisfied: transformers in /Users/bastienjossen/anaconda3/envs/pytorch_mps/lib/python3.11/site-packages (4.45.1)\r\n",
      "Requirement already satisfied: accelerate in /Users/bastienjossen/anaconda3/envs/pytorch_mps/lib/python3.11/site-packages (0.33.0)\r\n",
      "Requirement already satisfied: datasets in /Users/bastienjossen/anaconda3/envs/pytorch_mps/lib/python3.11/site-packages (3.0.1)\r\n",
      "Requirement already satisfied: bitsandbytes in /Users/bastienjossen/anaconda3/envs/pytorch_mps/lib/python3.11/site-packages (0.42.0)\r\n",
      "Requirement already satisfied: sentencepiece in /Users/bastienjossen/anaconda3/envs/pytorch_mps/lib/python3.11/site-packages (0.2.0)\r\n",
      "Requirement already satisfied: peft in /Users/bastienjossen/anaconda3/envs/pytorch_mps/lib/python3.11/site-packages (0.13.2)\r\n",
      "Requirement already satisfied: nbconvert==6.5.4 in /Users/bastienjossen/anaconda3/envs/pytorch_mps/lib/python3.11/site-packages (6.5.4)\r\n",
      "Requirement already satisfied: pypdf2==2 in /Users/bastienjossen/anaconda3/envs/pytorch_mps/lib/python3.11/site-packages (2.0.0)\r\n",
      "Requirement already satisfied: notebook-as-pdf in /Users/bastienjossen/anaconda3/envs/pytorch_mps/lib/python3.11/site-packages (0.5.0)\r\n",
      "Requirement already satisfied: seaborn in /Users/bastienjossen/anaconda3/envs/pytorch_mps/lib/python3.11/site-packages (0.13.2)\r\n",
      "Requirement already satisfied: protobuf in /Users/bastienjossen/anaconda3/envs/pytorch_mps/lib/python3.11/site-packages (4.25.3)\r\n",
      "Requirement already satisfied: lxml[html_clean] in /Users/bastienjossen/anaconda3/envs/pytorch_mps/lib/python3.11/site-packages (5.3.0)\r\n",
      "Requirement already satisfied: beautifulsoup4 in /Users/bastienjossen/anaconda3/envs/pytorch_mps/lib/python3.11/site-packages (from nbconvert==6.5.4) (4.12.3)\r\n",
      "Requirement already satisfied: bleach in /Users/bastienjossen/anaconda3/envs/pytorch_mps/lib/python3.11/site-packages (from nbconvert==6.5.4) (4.1.0)\r\n",
      "Requirement already satisfied: defusedxml in /Users/bastienjossen/anaconda3/envs/pytorch_mps/lib/python3.11/site-packages (from nbconvert==6.5.4) (0.7.1)\r\n",
      "Requirement already satisfied: entrypoints>=0.2.2 in /Users/bastienjossen/anaconda3/envs/pytorch_mps/lib/python3.11/site-packages (from nbconvert==6.5.4) (0.4)\r\n",
      "Requirement already satisfied: jinja2>=3.0 in /Users/bastienjossen/anaconda3/envs/pytorch_mps/lib/python3.11/site-packages (from nbconvert==6.5.4) (3.1.4)\r\n",
      "Requirement already satisfied: jupyter-core>=4.7 in /Users/bastienjossen/anaconda3/envs/pytorch_mps/lib/python3.11/site-packages (from nbconvert==6.5.4) (5.7.2)\r\n",
      "Requirement already satisfied: jupyterlab-pygments in /Users/bastienjossen/anaconda3/envs/pytorch_mps/lib/python3.11/site-packages (from nbconvert==6.5.4) (0.1.2)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/bastienjossen/anaconda3/envs/pytorch_mps/lib/python3.11/site-packages (from nbconvert==6.5.4) (2.1.3)\r\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in /Users/bastienjossen/anaconda3/envs/pytorch_mps/lib/python3.11/site-packages (from nbconvert==6.5.4) (0.8.4)\r\n",
      "Requirement already satisfied: nbclient>=0.5.0 in /Users/bastienjossen/anaconda3/envs/pytorch_mps/lib/python3.11/site-packages (from nbconvert==6.5.4) (0.8.0)\r\n",
      "Requirement already satisfied: nbformat>=5.1 in /Users/bastienjossen/anaconda3/envs/pytorch_mps/lib/python3.11/site-packages (from nbconvert==6.5.4) (5.9.2)\r\n",
      "Requirement already satisfied: packaging in /Users/bastienjossen/anaconda3/envs/pytorch_mps/lib/python3.11/site-packages (from nbconvert==6.5.4) (24.1)\r\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /Users/bastienjossen/anaconda3/envs/pytorch_mps/lib/python3.11/site-packages (from nbconvert==6.5.4) (1.5.0)\r\n",
      "Requirement already satisfied: pygments>=2.4.1 in /Users/bastienjossen/anaconda3/envs/pytorch_mps/lib/python3.11/site-packages (from nbconvert==6.5.4) (2.15.1)\r\n",
      "Requirement already satisfied: tinycss2 in /Users/bastienjossen/anaconda3/envs/pytorch_mps/lib/python3.11/site-packages (from nbconvert==6.5.4) (1.2.1)\r\n",
      "Requirement already satisfied: traitlets>=5.0 in /Users/bastienjossen/anaconda3/envs/pytorch_mps/lib/python3.11/site-packages (from nbconvert==6.5.4) (5.14.3)\r\n",
      "Requirement already satisfied: filelock in /Users/bastienjossen/anaconda3/envs/pytorch_mps/lib/python3.11/site-packages (from torch~=2.2.1) (3.13.1)\r\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/bastienjossen/anaconda3/envs/pytorch_mps/lib/python3.11/site-packages (from torch~=2.2.1) (4.11.0)\r\n",
      "Requirement already satisfied: sympy in /Users/bastienjossen/anaconda3/envs/pytorch_mps/lib/python3.11/site-packages (from torch~=2.2.1) (1.13.1)\r\n",
      "Requirement already satisfied: networkx in /Users/bastienjossen/anaconda3/envs/pytorch_mps/lib/python3.11/site-packages (from torch~=2.2.1) (3.2.1)\r\n",
      "Requirement already satisfied: fsspec in /Users/bastienjossen/anaconda3/envs/pytorch_mps/lib/python3.11/site-packages (from torch~=2.2.1) (2024.6.1)\r\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /Users/bastienjossen/anaconda3/envs/pytorch_mps/lib/python3.11/site-packages (from transformers) (0.24.6)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/bastienjossen/anaconda3/envs/pytorch_mps/lib/python3.11/site-packages (from transformers) (6.0.1)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/bastienjossen/anaconda3/envs/pytorch_mps/lib/python3.11/site-packages (from transformers) (2024.9.11)\r\n",
      "Requirement already satisfied: requests in /Users/bastienjossen/anaconda3/envs/pytorch_mps/lib/python3.11/site-packages (from transformers) (2.32.3)\r\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Users/bastienjossen/anaconda3/envs/pytorch_mps/lib/python3.11/site-packages (from transformers) (0.4.4)\r\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /Users/bastienjossen/anaconda3/envs/pytorch_mps/lib/python3.11/site-packages (from transformers) (0.20.0)\r\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/bastienjossen/anaconda3/envs/pytorch_mps/lib/python3.11/site-packages (from transformers) (4.66.5)\r\n",
      "Requirement already satisfied: psutil in /Users/bastienjossen/anaconda3/envs/pytorch_mps/lib/python3.11/site-packages (from accelerate) (5.9.0)\r\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /Users/bastienjossen/anaconda3/envs/pytorch_mps/lib/python3.11/site-packages (from datasets) (16.1.0)\r\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /Users/bastienjossen/anaconda3/envs/pytorch_mps/lib/python3.11/site-packages (from datasets) (0.3.6)\r\n",
      "Requirement already satisfied: pandas in /Users/bastienjossen/anaconda3/envs/pytorch_mps/lib/python3.11/site-packages (from datasets) (2.2.2)\r\n",
      "Requirement already satisfied: xxhash in /Users/bastienjossen/anaconda3/envs/pytorch_mps/lib/python3.11/site-packages (from datasets) (2.0.2)\r\n",
      "Requirement already satisfied: multiprocess in /Users/bastienjossen/anaconda3/envs/pytorch_mps/lib/python3.11/site-packages (from datasets) (0.70.14)\r\n",
      "Requirement already satisfied: aiohttp in /Users/bastienjossen/anaconda3/envs/pytorch_mps/lib/python3.11/site-packages (from datasets) (3.10.5)\r\n",
      "Requirement already satisfied: scipy in /Users/bastienjossen/anaconda3/envs/pytorch_mps/lib/python3.11/site-packages (from bitsandbytes) (1.13.1)\r\n",
      "Requirement already satisfied: lxml-html-clean in /Users/bastienjossen/anaconda3/envs/pytorch_mps/lib/python3.11/site-packages (from lxml[html_clean]) (0.3.1)\r\n",
      "Requirement already satisfied: pyppeteer in /Users/bastienjossen/anaconda3/envs/pytorch_mps/lib/python3.11/site-packages (from notebook-as-pdf) (2.0.0)\r\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /Users/bastienjossen/anaconda3/envs/pytorch_mps/lib/python3.11/site-packages (from seaborn) (3.9.2)\r\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Users/bastienjossen/anaconda3/envs/pytorch_mps/lib/python3.11/site-packages (from aiohttp->datasets) (2.4.0)\r\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/bastienjossen/anaconda3/envs/pytorch_mps/lib/python3.11/site-packages (from aiohttp->datasets) (1.2.0)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/bastienjossen/anaconda3/envs/pytorch_mps/lib/python3.11/site-packages (from aiohttp->datasets) (23.1.0)\r\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/bastienjossen/anaconda3/envs/pytorch_mps/lib/python3.11/site-packages (from aiohttp->datasets) (1.4.0)\r\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/bastienjossen/anaconda3/envs/pytorch_mps/lib/python3.11/site-packages (from aiohttp->datasets) (6.0.4)\r\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/bastienjossen/anaconda3/envs/pytorch_mps/lib/python3.11/site-packages (from aiohttp->datasets) (1.11.0)\r\n",
      "Requirement already satisfied: platformdirs>=2.5 in /Users/bastienjossen/anaconda3/envs/pytorch_mps/lib/python3.11/site-packages (from jupyter-core>=4.7->nbconvert==6.5.4) (3.10.0)\r\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/bastienjossen/anaconda3/envs/pytorch_mps/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.2.0)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/bastienjossen/anaconda3/envs/pytorch_mps/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.11.0)\r\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/bastienjossen/anaconda3/envs/pytorch_mps/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.51.0)\r\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/bastienjossen/anaconda3/envs/pytorch_mps/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.4)\r\n",
      "Requirement already satisfied: pillow>=8 in /Users/bastienjossen/anaconda3/envs/pytorch_mps/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (10.4.0)\r\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/bastienjossen/anaconda3/envs/pytorch_mps/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.1.2)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/bastienjossen/anaconda3/envs/pytorch_mps/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0.post0)\r\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in /Users/bastienjossen/anaconda3/envs/pytorch_mps/lib/python3.11/site-packages (from nbclient>=0.5.0->nbconvert==6.5.4) (8.6.0)\r\n",
      "Requirement already satisfied: fastjsonschema in /Users/bastienjossen/anaconda3/envs/pytorch_mps/lib/python3.11/site-packages (from nbformat>=5.1->nbconvert==6.5.4) (2.16.2)\r\n",
      "Requirement already satisfied: jsonschema>=2.6 in /Users/bastienjossen/anaconda3/envs/pytorch_mps/lib/python3.11/site-packages (from nbformat>=5.1->nbconvert==6.5.4) (4.19.2)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/bastienjossen/anaconda3/envs/pytorch_mps/lib/python3.11/site-packages (from pandas->datasets) (2024.1)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/bastienjossen/anaconda3/envs/pytorch_mps/lib/python3.11/site-packages (from pandas->datasets) (2023.3)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/bastienjossen/anaconda3/envs/pytorch_mps/lib/python3.11/site-packages (from requests->transformers) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/bastienjossen/anaconda3/envs/pytorch_mps/lib/python3.11/site-packages (from requests->transformers) (3.7)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/bastienjossen/anaconda3/envs/pytorch_mps/lib/python3.11/site-packages (from requests->transformers) (1.26.20)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/bastienjossen/anaconda3/envs/pytorch_mps/lib/python3.11/site-packages (from requests->transformers) (2024.8.30)\r\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/bastienjossen/anaconda3/envs/pytorch_mps/lib/python3.11/site-packages (from beautifulsoup4->nbconvert==6.5.4) (2.5)\r\n",
      "Requirement already satisfied: six>=1.9.0 in /Users/bastienjossen/anaconda3/envs/pytorch_mps/lib/python3.11/site-packages (from bleach->nbconvert==6.5.4) (1.16.0)\r\n",
      "Requirement already satisfied: webencodings in /Users/bastienjossen/anaconda3/envs/pytorch_mps/lib/python3.11/site-packages (from bleach->nbconvert==6.5.4) (0.5.1)\r\n",
      "Requirement already satisfied: appdirs<2.0.0,>=1.4.3 in /Users/bastienjossen/anaconda3/envs/pytorch_mps/lib/python3.11/site-packages (from pyppeteer->notebook-as-pdf) (1.4.4)\r\n",
      "Requirement already satisfied: importlib-metadata>=1.4 in /Users/bastienjossen/anaconda3/envs/pytorch_mps/lib/python3.11/site-packages (from pyppeteer->notebook-as-pdf) (7.0.1)\r\n",
      "Requirement already satisfied: pyee<12.0.0,>=11.0.0 in /Users/bastienjossen/anaconda3/envs/pytorch_mps/lib/python3.11/site-packages (from pyppeteer->notebook-as-pdf) (11.1.1)\r\n",
      "Requirement already satisfied: websockets<11.0,>=10.0 in /Users/bastienjossen/anaconda3/envs/pytorch_mps/lib/python3.11/site-packages (from pyppeteer->notebook-as-pdf) (10.4)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/bastienjossen/anaconda3/envs/pytorch_mps/lib/python3.11/site-packages (from sympy->torch~=2.2.1) (1.3.0)\r\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/bastienjossen/anaconda3/envs/pytorch_mps/lib/python3.11/site-packages (from importlib-metadata>=1.4->pyppeteer->notebook-as-pdf) (3.17.0)\r\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /Users/bastienjossen/anaconda3/envs/pytorch_mps/lib/python3.11/site-packages (from jsonschema>=2.6->nbformat>=5.1->nbconvert==6.5.4) (2023.7.1)\r\n",
      "Requirement already satisfied: referencing>=0.28.4 in /Users/bastienjossen/anaconda3/envs/pytorch_mps/lib/python3.11/site-packages (from jsonschema>=2.6->nbformat>=5.1->nbconvert==6.5.4) (0.30.2)\r\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /Users/bastienjossen/anaconda3/envs/pytorch_mps/lib/python3.11/site-packages (from jsonschema>=2.6->nbformat>=5.1->nbconvert==6.5.4) (0.10.6)\r\n",
      "Requirement already satisfied: pyzmq>=23.0 in /Users/bastienjossen/anaconda3/envs/pytorch_mps/lib/python3.11/site-packages (from jupyter-client>=6.1.12->nbclient>=0.5.0->nbconvert==6.5.4) (25.1.2)\r\n",
      "Requirement already satisfied: tornado>=6.2 in /Users/bastienjossen/anaconda3/envs/pytorch_mps/lib/python3.11/site-packages (from jupyter-client>=6.1.12->nbclient>=0.5.0->nbconvert==6.5.4) (6.4.1)\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "e5ba03fea5ba1506",
   "metadata": {
    "tags": [
     "hide-cell"
    ],
    "ExecuteTime": {
     "end_time": "2024-10-30T16:32:47.617695Z",
     "start_time": "2024-10-30T16:32:46.186392Z"
    }
   },
   "source": [
    "# This will be (most) of the packages that you will need during the lab.\n",
    "# Make sure to run this cell each time you (re-)start the IPython kernel.\n",
    "import textwrap\n",
    "import warnings\n",
    "from importlib import metadata\n",
    "\n",
    "import datasets\n",
    "import jinja2\n",
    "from typing import *\n",
    "from collections import defaultdict\n",
    "import itertools\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "from IPython.display import display, HTML, Markdown\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "print(\"Imports done!\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports done!\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "6965ab32696e4800",
   "metadata": {
    "tags": [
     "hide-cell"
    ],
    "ExecuteTime": {
     "end_time": "2024-10-30T17:05:17.581168Z",
     "start_time": "2024-10-30T17:05:17.566523Z"
    }
   },
   "source": [
    "def display_dataset_description(name: str, dataset: datasets.DatasetDict):\n",
    "    split_info = []\n",
    "    for k, ds in dataset.items():\n",
    "        split_info.append(f\"<tr><td><strong>{k.capitalize()} Samples:</strong></td><td>{len(ds)}</td></tr>\")\n",
    "    html_content = f\"\"\"\n",
    "    <h2>Dataset info</h2>\n",
    "    <table>\n",
    "        <tr><td><strong>Dataset Name:</strong></td><td>{name}</td></tr>\n",
    "        {\"<br>\".join(split_info)}\n",
    "    </table>\n",
    "    \"\"\"\n",
    "    \n",
    "    # Display the output in the notebook\n",
    "    display(HTML(html_content))\n",
    "\n",
    "def get_available_device():\n",
    "    \"\"\"Helper method to find best possible hardware to run\n",
    "    Returns:\n",
    "\n",
    "    \"\"\"\n",
    "    # Check if CUDA is available\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\"), \"cuda\"\n",
    "    \n",
    "    # Check if ROCm is available\n",
    "    if torch.version.hip is not None and torch.backends.mps.is_available():\n",
    "        return torch.device(\"rocm\"), \"rocm\"\n",
    "    \n",
    "    # Check if MPS (Apple Silicon) is available\n",
    "    if torch.backends.mps.is_available():\n",
    "        return torch.device('cpu'), \"mps\"\n",
    "    \n",
    "    \n",
    "    # Fall back to CPU\n",
    "    return torch.device(\"cpu\"), \"cpu\"\n",
    "\n",
    "def get_installed_version(package_name):\n",
    "    with warnings.catch_warnings():\n",
    "        # Supress warnings from packages that have missing attributes that metadata will complain about.\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        distribution = metadata.Distribution()\n",
    "        try:\n",
    "            return distribution.from_name(package_name).version\n",
    "        except metadata.PackageNotFoundError:\n",
    "            return \"Not installed\"\n",
    "\n",
    "\n",
    "def display_configuration():\n",
    "    # Check device info\n",
    "    device, backend = get_available_device()\n",
    "\n",
    "    # Torch version\n",
    "    torch_version = torch.__version__\n",
    "\n",
    "    # HuggingFace Transformers version\n",
    "    transformers_ver = transformers.__version__\n",
    "\n",
    "    # BitsAndBytes version (if available)\n",
    "    bitsandbytes_version = get_installed_version(\"bitsandbytes\")\n",
    "\n",
    "    # Check for GPU-specific details if CUDA or ROCm is available\n",
    "    if device.type == \"cuda\":\n",
    "        cuda_device_count = torch.cuda.device_count()\n",
    "        cuda_device_name = torch.cuda.get_device_name(0)\n",
    "        cuda_version = torch.version.cuda\n",
    "    elif device.type == \"rocm\":\n",
    "        cuda_device_count = torch.cuda.device_count()\n",
    "        cuda_device_name = torch.cuda.get_device_name(0)\n",
    "        cuda_version = torch.version.hip\n",
    "    else:\n",
    "        cuda_device_count = 0\n",
    "        cuda_device_name = \"N/A\"\n",
    "        cuda_version = \"N/A\"\n",
    "\n",
    "    # Prepare HTML formatted output for better display in a notebook\n",
    "    html_content = f\"\"\"\n",
    "    <h2>System Configuration</h2>\n",
    "    <table>\n",
    "        <tr><td><strong>PyTorch version:</strong></td><td>{torch_version}</td></tr>\n",
    "        <tr><td><strong>Device:</strong></td><td>{device} (Backend: {backend})</td></tr>\n",
    "        <tr><td><strong>CUDA/ROCm version:</strong></td><td>{cuda_version}</td></tr>\n",
    "        <tr><td><strong>GPU count:</strong></td><td>{cuda_device_count}</td></tr>\n",
    "        <tr><td><strong>GPU name:</strong></td><td>{cuda_device_name}</td></tr>\n",
    "        <tr><td><strong>Hugging Face Transformers version:</strong></td><td>{transformers_ver}</td></tr>\n",
    "        <tr><td><strong>BitsAndBytes version:</strong></td><td>{bitsandbytes_version}</td></tr>\n",
    "    </table>\n",
    "    \"\"\"\n",
    "    \n",
    "    # Display the output in the notebook\n",
    "    display(HTML(html_content))\n",
    "\n",
    "\n",
    "def label_mapper(label: int) -> str:\n",
    "    \"\"\"Map label from int to string!\"\"\"\n",
    "    return ['Negative', \"Positive\"][label]\n",
    "\n",
    "def simple_truncate_text(row, max_length=50, tokenizer: transformers.PreTrainedTokenizerFast = None):\n",
    "    \"\"\"Example of a simple truncation method text, based on token count.\n",
    "    \n",
    "    You might want to perform 'smarter' truncation / summarization as a level, instead of simply cutting of after `max_length` tokens.\n",
    "    \n",
    "    Examples:\n",
    "        You might want to partially-apply the function, to provide a different tokenizer:\n",
    "        ```python3\n",
    "        from functools import partial\n",
    "        some_other_tokenizer = transformers.AutoTokenizer.from_pretrained('your_fave_tokenizer')\n",
    "        partial_simple_truncate = partial(simple_truncate_text, tokenizer=some_other_tokenizer)\n",
    "        ```\n",
    "    Args:\n",
    "        row (datasets....): Single instance or row of dataset.\n",
    "    \n",
    "    Keyword Args:\n",
    "        max_length (int, 150): the maximum length of text to be processed. Defaults to 150.\n",
    "        tokenizer (transformers.PreTrainedTokenizer, `fast_tokenizer`): the tokenizer to use. Defaults to `fast_tokenizer`.\n",
    "    \n",
    "    Notes:\n",
    "        This function requires all cells above to be run.\n",
    "    \"\"\"\n",
    "    token_representation = tokenizer.batch_encode_plus(row['text'], max_length=max_length, truncation=True)['input_ids']\n",
    "    text_representation = tokenizer.batch_decode(token_representation, skip_special_tokens=True)\n",
    "    row['text'] = text_representation\n",
    "    return row\n",
    "\n",
    "def generate(\n",
    "        input_text: str,\n",
    "        tokenizer: transformers.PreTrainedTokenizer,\n",
    "        model: transformers.PreTrainedModel,\n",
    "        generation_config: transformers.GenerationConfig,\n",
    ") -> str:\n",
    "    \"\"\"Helper method to generate a sample from the model, pre-conditioned on the input-text 'Prompt'.\n",
    "    \n",
    "    Args:\n",
    "        input_text (str): Input text to be conditioned on.\n",
    "        tokenizer (transformers.PreTrainedTokenizer): Tokenizer corresponding to the model provided.\n",
    "        model (transformers.PreTrainedModel): Pre-trained model to perform text generation with.\n",
    "        \n",
    "    Returns:\n",
    "        Generate text by the pre-conditioned model.\n",
    "    \"\"\"\n",
    "    \n",
    "    input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(model.device)\n",
    "    outputs = model.generate(input_ids=input_ids, generation_config=generation_config)\n",
    "    result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return result"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "6fec69498a66f823",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-30T17:05:32.346763Z",
     "start_time": "2024-10-30T17:05:32.339319Z"
    }
   },
   "source": [
    "# Retrieve 'best' backend and device\n",
    "device, backend = get_available_device()\n",
    "\n",
    "# Default bfloat16, because there is a lot of optimization\n",
    "dtype = torch.bfloat16\n",
    "\n",
    "# Optional bits-and-bytes configuration for additional quantization.\n",
    "bab_conf = None\n",
    "if backend == 'cuda':\n",
    "    # If you want, you can further quantize on CUDA devices (linux and WSL)\n",
    "    # However, this is more for you to explore than anything else.\n",
    "    bab_conf =  transformers.BitsAndBytesConfig(\n",
    "        load_in_8bit=False\n",
    "    )\n",
    "\n",
    "# Call the display_configuration() function in your Jupyter notebook to show the configuration\n",
    "display_configuration()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <h2>System Configuration</h2>\n",
       "    <table>\n",
       "        <tr><td><strong>PyTorch version:</strong></td><td>2.2.2</td></tr>\n",
       "        <tr><td><strong>Device:</strong></td><td>cpu (Backend: mps)</td></tr>\n",
       "        <tr><td><strong>CUDA/ROCm version:</strong></td><td>N/A</td></tr>\n",
       "        <tr><td><strong>GPU count:</strong></td><td>0</td></tr>\n",
       "        <tr><td><strong>GPU name:</strong></td><td>N/A</td></tr>\n",
       "        <tr><td><strong>Hugging Face Transformers version:</strong></td><td>4.45.1</td></tr>\n",
       "        <tr><td><strong>BitsAndBytes version:</strong></td><td>0.42.0</td></tr>\n",
       "    </table>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "id": "5fde36c45670e1e",
   "metadata": {},
   "source": [
    "# Part 1: Preparing all the things\n",
    "\n",
    "Before we get started with our small lab experiment, we need to make sure that everything is prepared. Let's get started with setting up a small language model, and and loading and preparing the data.\n",
    "\n",
    "Recall from the lecture that this consists of the following 'recipe'.\n",
    "\n",
    "1. Load the model and data.\n",
    "   1. Load pre-trained or fine-tuned model\n",
    "   2. Load dataset and tokenize\n",
    "2. Run the data through the model\n",
    "3. Perform experiments (+ Analysis)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b63a69f16551143",
   "metadata": {},
   "source": [
    "## Step 1: Preparing The Model\n",
    "Loading the model and see that it work, we will use the Flan-T5 model by Google / DeepMind. This model is tiny and should be fast enough even on lower powered hardware!\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "9cb66d1eca874f81",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-30T17:05:56.709188Z",
     "start_time": "2024-10-30T17:05:55.555272Z"
    }
   },
   "source": [
    "# Create tokenizer for flan family\n",
    "family: str = \"google/flan-t5\"\n",
    "\n",
    "# For the Lab we will use a small model, just to provide some insight into usability.\n",
    "model: str = f\"-small\" # # '-base', '-large'\n",
    "\n",
    "model_name: str = f\"{family}{model}\"\n",
    "# Create tokenizer\n",
    "# import os\n",
    "# HF_TOKEN='hf_...'\n",
    "tokenizer: T5Tokenizer = T5Tokenizer.from_pretrained(model_name, legacy=False)\n",
    "dtype = torch.bfloat16 # torch.float16\n",
    "# Instantate model and load to the correct device.\n",
    "model: T5ForConditionalGeneration = T5ForConditionalGeneration.from_pretrained(\n",
    "    pretrained_model_name_or_path=model_name,\n",
    "    device_map=device,\n",
    "    torch_dtype=dtype,\n",
    ")\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bastienjossen/anaconda3/envs/pytorch_mps/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "id": "44cf2cd6230b091b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-30T17:07:32.995106Z",
     "start_time": "2024-10-30T17:07:31.316262Z"
    }
   },
   "source": [
    "# Here we check that everything is working, note that the story should be quite bad, as T5 is not really trained to tell us stories.\n",
    "\n",
    "input_text = \"Write a story about a dog and a boy playing with a ball on a boat with sailors.\"\n",
    "# N.B., this might differ slighty for different versions of libraries.\n",
    "expected_response = 'The dog and the boy are playing with a ball on a boat. They are chasing each other in the water. The dog is chasing the other dog. The dog is chasing the other dog. The dog is chasing'\n",
    "\n",
    "deterministic_config = transformers.GenerationConfig(do_sample=False, max_length=50, min_length=25, repetition_penalty=1.19)\n",
    "deterministic_reponse = generate(\n",
    "            input_text=input_text,\n",
    "            tokenizer=tokenizer,\n",
    "            model=model,\n",
    "            generation_config=deterministic_config\n",
    "        )\n",
    "\n",
    "do_sample_config = transformers.GenerationConfig(do_sample=True, max_length=50, min_length=25, repetition_penalty=1.19)\n",
    "random_response = generate(\n",
    "            input_text=input_text,\n",
    "            tokenizer=tokenizer,\n",
    "            model=model,\n",
    "            generation_config=do_sample_config\n",
    "        )"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "id": "75133491d38afc8c",
   "metadata": {
    "tags": [
     "hide-cell"
    ],
    "ExecuteTime": {
     "end_time": "2024-10-30T17:07:36.374698Z",
     "start_time": "2024-10-30T17:07:36.368345Z"
    }
   },
   "source": [
    "display(\n",
    "    Markdown(\"### Generated text by the LLM\"),\n",
    "    Markdown(f\"> {input_text}\"),\n",
    "    Markdown('*Deterministic Generation*'),\n",
    "    Markdown(\n",
    "        textwrap.dedent(\n",
    "            f\"\"\"\\\n",
    "            | Deterministic           | Deterministic (response) | Equal (True/False)                            |\n",
    "            |-------------------------|--------------------------|-----------------------------------------------|\n",
    "            | {expected_response}     | {deterministic_reponse}  | {expected_response == deterministic_reponse}  |\n",
    "            \"\"\"\n",
    "        )\n",
    "    ),\n",
    "    Markdown('*Random Generation*'),\n",
    "    Markdown(\n",
    "        textwrap.dedent(\n",
    "            f\"\"\"\\\n",
    "            | Deterministic           | Random (response)        | Equal (True/False)                           |\n",
    "            |-------------------------|--------------------------|----------------------------------------------|\n",
    "            | {expected_response}     | {random_response}        |{random_response == deterministic_reponse}    |\n",
    "            \"\"\"\n",
    "        )\n",
    "    )    \n",
    ")"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "### Generated text by the LLM"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "> Write a story about a dog and a boy playing with a ball on a boat with sailors."
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "*Deterministic Generation*"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "| Deterministic           | Deterministic (response) | Equal (True/False)                            |\n|-------------------------|--------------------------|-----------------------------------------------|\n| The dog and the boy are playing with a ball on a boat. They are chasing each other in the water. The dog is chasing the other dog. The dog is chasing the other dog. The dog is chasing     | The dog and the boy are playing with a ball on a boat. They are chasing each other in the water. The dog is chasing the other dog. The dog is chasing the other dog. The dog is chasing  | True  |\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "*Random Generation*"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "| Deterministic           | Random (response)        | Equal (True/False)                           |\n|-------------------------|--------------------------|----------------------------------------------|\n| The dog and the boy are playing with a ball on a boat. They are chasing each other in the water. The dog is chasing the other dog. The dog is chasing the other dog. The dog is chasing     | He got off to a good start with the ball but he found it too boring. Then he took his time to make sure it was the right ball, just like the fish he had on both boats. Eventually        |False    |\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "id": "e4d787247f9559eb",
   "metadata": {},
   "source": [
    "## Step 1.5 Preparing the Data.\n",
    "\n",
    "As we will be working with a Semtiment 'classification' task, as the only labels are `Postive` (`1`) or `Negative` (`0`). First, we will need to load the appropriate dataset (`standfordnlp/imbd`), which contains movie reviews and their respective label. During the rest of the lab, we will further investigate how to do pre-processing of the data, run (different types of) inference, and perform fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "id": "a727f6b877435832",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-30T17:08:13.289920Z",
     "start_time": "2024-10-30T17:08:10.053095Z"
    }
   },
   "source": [
    "# Define dataset name\n",
    "data_name: str = 'stanfordnlp/imdb'\n",
    "\n",
    "# Load dataset, and assign splits to variables\n",
    "dataset: datasets.DatasetDict = datasets.load_dataset(data_name)\n",
    "train_set: datasets.Dataset = dataset['train']\n",
    "test_set: datasets.Dataset = dataset['test']\n",
    "# This unsupervised split is not used in the rest of the notebook.\n",
    "unsup: datasets.Dataset = dataset['unsupervised']\n",
    "\n",
    "# Give an overview\n",
    "display_dataset_description(data_name, dataset)\n"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <h2>Dataset info</h2>\n",
       "    <table>\n",
       "        <tr><td><strong>Dataset Name:</strong></td><td>stanfordnlp/imdb</td></tr>\n",
       "        <tr><td><strong>Train Samples:</strong></td><td>25000</td></tr><br><tr><td><strong>Test Samples:</strong></td><td>25000</td></tr><br><tr><td><strong>Unsupervised Samples:</strong></td><td>50000</td></tr>\n",
       "    </table>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "id": "765a3d7cad2c7b5b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-30T17:08:20.440029Z",
     "start_time": "2024-10-30T17:08:20.435058Z"
    }
   },
   "source": [
    "sample = train_set[1231]\n",
    "review_1, label_1 = sample['text'], label_mapper(sample['label'])\n",
    "sample = train_set[15442]\n",
    "review_2, label_2 = sample['text'], label_mapper(sample['label'])\n",
    "\n",
    "\n",
    "display(\n",
    "    Markdown(\n",
    "        textwrap.dedent(f\"\"\"\\\n",
    "        | *Example*                 | Label     |\n",
    "        |:--------------------------|:---------:|\n",
    "        | {review_1}                | {label_1} |\n",
    "        | {review_2}                | {label_2} |\n",
    "        \"\"\")\n",
    "    )\n",
    ")"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "| *Example*                 | Label     |\n|:--------------------------|:---------:|\n| VERY dull, obvious, tedious Exorcist rip-off featuring a Doberman with red eyes - that's the extent of the special effects in this made-for-tv cheapie. Richard Crenna is about as animate as a chew toy. Very 70's dress & music only add to the torture. Should put you to sleep almost as fast as \"The Corpse Vanishes\", or \"The Blue Hand\". Practically worthless. MooCow says eaghhh what a stinky dog! :=8P                | Negative |\n| This review is for the extended cut of this movie.<br /><br />I first watched Dragon Lord when I bought it on DVD many years ago. I always liked this movie and you can read some of the more positive reviews of it to get the general idea.<br /><br />That being said. I've always found the storyline a bit confusing. The movie is, after all, a love story. And it always seemed strange to me that a love story should end with a 20 minute fight scene.<br /><br />Well, in the extended version this is no longer so. The old \"original\" version begins off with a huge barrel-climb/rugby-like sequence which is the new ending sequence in the extended version. The opening sequence is Dragon(Jackie Chan) hanging around his house and pretending to be training and reciting whenever his father is around.<br /><br />Other sequenced have also been shift or prolonged in the extended cut and the story makes a lot more sense when you watch it. The pacing is also better and overall it just works better. It feels more like a love story and doesn't leave you asking questions about why it ends so drastically and dramatically as the regular version does.<br /><br />I suggest everyone who is a Hong-Kong cinema, or just plain Jackie Chan fanatic to get a hold of the extended version and watch the movie the way it was originally intended.(Or at least that's how I think it was intended. Why else would they make it and rearrange some of the scenes) When I was done watching it, I felt like I had watched a completely new Jackie Chan movie although most of the sequences were the same.                | Positive |\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "id": "9e889eb982bd5568",
   "metadata": {},
   "source": [
    "Next we will create some dataloader to ensure that we can quickly load data into the model, making the rest of the cells load a little faster.\n",
    "\n",
    "Let's also define some library functions, that we can use to calculate the performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "id": "a52aba2cc1c8819d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-30T17:11:02.343849Z",
     "start_time": "2024-10-30T17:11:02.342112Z"
    }
   },
   "source": [
    "# First determine some hyper-parameters, this should be fine on even a small model and CPU only\n",
    "\n",
    "# If you have a GPU / powerful machine, feel free to increase the following\n",
    "batch_size = 1\n",
    "test_samples = 1000\n",
    "max_iterations = test_samples // batch_size"
   ],
   "outputs": [],
   "execution_count": 17
  },
  {
   "cell_type": "markdown",
   "id": "c0a3b324d7a9e3b2",
   "metadata": {},
   "source": [
    "# Approach 1: Simple Prompting\n",
    "\n",
    "Rather than going straight into a complex solution, let's first see what we can achieve by letting the model predict the output.\n",
    "\n",
    "\n",
    "> Note, I annotate the 'steps' in comments. There might be code sections that we will fill in during the lab, annotated with.\n",
    "\n",
    "\n",
    "```python\n",
    "# YOUR CODE GOES HERE!\n",
    "# END OF YOUR CODE!\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "bdaae94fd14c7426",
   "metadata": {
    "tags": [
     "hide-cell"
    ],
    "ExecuteTime": {
     "end_time": "2024-10-30T17:11:02.999791Z",
     "start_time": "2024-10-30T17:11:02.995655Z"
    }
   },
   "source": [
    "\n",
    "def simple_prompt_function(batch):\n",
    "    \"\"\"Simple prompt preparation function.\"\"\"\n",
    "    stringified_representation = list(map(lambda x: f\"Positive/Negative? {x})\", batch['text']))\n",
    "    batch['text'] = stringified_representation\n",
    "    return batch\n",
    "\n",
    "def simple_template_function(batch, template=None):\n",
    "    \"\"\"Mapping function using a template. Note, we will show in the lab to set this up.\"\"\"\n",
    "    stringified_representation = [template.render(review=review) for review in batch['text']]\n",
    "    batch['text'] = stringified_representation\n",
    "    return batch"
   ],
   "outputs": [],
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "id": "8f8ff65650b3f67f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-30T17:11:04.187937Z",
     "start_time": "2024-10-30T17:11:03.528117Z"
    }
   },
   "source": [
    "\n",
    "sub_sampled_set = test_set.shuffle(seed=123).take(test_samples)\n",
    "tokenized_eval_dataset = (\n",
    "    sub_sampled_set\n",
    "    .map(simple_prompt_function, batched=True)\n",
    "    .map(lambda batch: tokenizer(batch['text']))\n",
    ")\n",
    "tokenized_eval_dataset.set_format('torch', columns=['input_ids', 'label', 'attention_mask'])\n",
    "# TODO: Let's re-write to use a template!\n",
    "\n",
    "\n",
    "# YOUR CODE GOES HERE\n",
    "...\n",
    "# END OF YOUR CODE\n",
    "\n",
    "# Display the de-tokenized text\n",
    "display(\n",
    "    Markdown('### What the model `sees`'),\n",
    "    Markdown(\n",
    "        f\"\"\"{tokenized_eval_dataset[0]['input_ids'][:100]}...\"\"\"\n",
    "    ),\n",
    "    Markdown('### What we would `see`'),\n",
    "    Markdown(\n",
    "        f\"\"\"{tokenizer.decode(tokenized_eval_dataset[0]['input_ids'], skip_special_tokens=True)}\"\"\"\n",
    "    )\n",
    ")"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1c7769e6a391475e96806a633f2afdbc"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bd97a58dffb54598be18fbc9061e96d4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "### What the model `sees`"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "tensor([24972,    87,   567,    15,   122,  1528,    58, 10406,    25,   103,\n            6,   278,    31,    17,  1190,  3355,   116,    25,   317,     8,\n         1974,    19,   147,    55, 15280,   300,    21,     8,   166, 11587,\n           13, 11893,    42,    25,    31,   195,   310,  3041,   424,    55,\n          101,  1509,    48,  1974,    44,     8,  1138,  2132,     9,     9,\n          107,   814,  3994,    11,   816,    34,    47,     8,   200,    13,\n            8,  7292,     5,   707,    15,    63,     9,  1620,    49,    19,\n            3,     9, 23092,   310,     6,    59,   163,   250,    13,   160,\n          821,     6,    68,   250,   255,    54,  3197,    34,   326,    78,\n          623,   756,     8,  1591,     5,   486,     8,  1593,   184,   188])..."
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "### What we would `see`"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "Positive/Negative? Whatever you do, don't stop watching when you think the movie is over! Hang around for the first batch of credits or you'll really miss something! We saw this movie at the Savanaah film festival and thought it was the best of the bunch. Dreya Weber is a marvel really, not only because of her performance, but because she can pull it off so far above the ground. At the Q&A she said there were no wires or effects, so everything you see is really her going for it. Addie couldn't make it to the festival because she was dancing with Madonna. She was excellent and, my gawd, so beautiful. I was amazed that the film went over so well with the blue haired lady crowd, but there you have it, Savannnah isn't a backwater.)"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 19
  },
  {
   "cell_type": "markdown",
   "id": "af5d47c35bbf7fd5",
   "metadata": {},
   "source": [
    "# Part 2: Running Inference\n",
    "\n",
    "Now that we got the setup out of way, we can start 'running experiments'. In short this boils down to performing 3 steps;\n",
    "\n",
    "1. Choosing your hyper-parameters and choosing appropriate levels\n",
    "2. Getting a script ready to run your experiments\n",
    "3. ** Run the experiments.** (Or, an excellent time to get coffee :P)\n",
    "\n",
    "This part of the lab will focus on that last point, to ensure that you can run your experiment efficiently, in the tutorial we are going to fix some issues with the code below, and make it run *fast*er (with some caveats)."
   ]
  },
  {
   "cell_type": "code",
   "id": "89a8a18142c5a73",
   "metadata": {
    "tags": [
     "hide-cell"
    ],
    "ExecuteTime": {
     "end_time": "2024-10-31T08:04:35.339350Z",
     "start_time": "2024-10-31T08:04:35.331338Z"
    }
   },
   "source": [
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_fn(data_loader, evaluate_model=None) -> Tuple[List[List[int]], List[List[int]]]:\n",
    "    labels_list = []\n",
    "    prediction_list = []\n",
    "    for batch in tqdm(data_loader):\n",
    "        input_ids, attention_mask, label = batch['input_ids'].to(evaluate_model.device), batch['attention_mask'].to(evaluate_model.device), batch['label'].to(evaluate_model.device)\n",
    "        outputs = evaluate_model.generate(\n",
    "          input_ids=input_ids,\n",
    "          attention_mask=attention_mask,\n",
    "          max_length=3,\n",
    "          do_sample=False,\n",
    "        )\n",
    "        prediction = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "        prediction_list.append(prediction)\n",
    "        labels_list.append(label.tolist())\n",
    "    return prediction_list, labels_list"
   ],
   "outputs": [],
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "id": "84225092638034ce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T08:41:27.222859Z",
     "start_time": "2024-10-31T08:41:26.942240Z"
    }
   },
   "source": [
    "\n",
    "# YOUR CODE GOES HERE\n",
    "...\n",
    "# END OF YOUR CODE \n",
    "# 5. Set the format of the dataset to PyTorch Tensors\n",
    "eval_loader = torch.utils.data.DataLoader(\n",
    "    tokenized_eval_dataset,\n",
    "    batch_size=100,\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "\n",
    "# And run the evaluation\n",
    "predictions_list, labels_list = evaluate_fn(eval_loader, evaluate_model=model)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4eed19d4bed34d039a7a278169e30dfe"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [193] at entry 0 and [467] at entry 1",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[24], line 13\u001B[0m\n\u001B[1;32m      5\u001B[0m eval_loader \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mutils\u001B[38;5;241m.\u001B[39mdata\u001B[38;5;241m.\u001B[39mDataLoader(\n\u001B[1;32m      6\u001B[0m     tokenized_eval_dataset,\n\u001B[1;32m      7\u001B[0m     batch_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m100\u001B[39m,\n\u001B[1;32m      8\u001B[0m     shuffle\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[1;32m      9\u001B[0m )\n\u001B[1;32m     12\u001B[0m \u001B[38;5;66;03m# And run the evaluation\u001B[39;00m\n\u001B[0;32m---> 13\u001B[0m predictions_list, labels_list \u001B[38;5;241m=\u001B[39m evaluate_fn(eval_loader, evaluate_model\u001B[38;5;241m=\u001B[39mmodel)\n",
      "File \u001B[0;32m~/anaconda3/envs/pytorch_mps/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001B[0m, in \u001B[0;36mcontext_decorator.<locals>.decorate_context\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    112\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[1;32m    113\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecorate_context\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m    114\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[0;32m--> 115\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "Cell \u001B[0;32mIn[22], line 5\u001B[0m, in \u001B[0;36mevaluate_fn\u001B[0;34m(data_loader, evaluate_model)\u001B[0m\n\u001B[1;32m      3\u001B[0m labels_list \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m      4\u001B[0m prediction_list \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m----> 5\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m batch \u001B[38;5;129;01min\u001B[39;00m tqdm(data_loader):\n\u001B[1;32m      6\u001B[0m     input_ids, attention_mask, label \u001B[38;5;241m=\u001B[39m batch[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124minput_ids\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mto(evaluate_model\u001B[38;5;241m.\u001B[39mdevice), batch[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mattention_mask\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mto(evaluate_model\u001B[38;5;241m.\u001B[39mdevice), batch[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlabel\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mto(evaluate_model\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[1;32m      7\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m evaluate_model\u001B[38;5;241m.\u001B[39mgenerate(\n\u001B[1;32m      8\u001B[0m       input_ids\u001B[38;5;241m=\u001B[39minput_ids,\n\u001B[1;32m      9\u001B[0m       attention_mask\u001B[38;5;241m=\u001B[39mattention_mask,\n\u001B[1;32m     10\u001B[0m       max_length\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m3\u001B[39m,\n\u001B[1;32m     11\u001B[0m       do_sample\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[1;32m     12\u001B[0m     )\n",
      "File \u001B[0;32m~/anaconda3/envs/pytorch_mps/lib/python3.11/site-packages/tqdm/notebook.py:250\u001B[0m, in \u001B[0;36mtqdm_notebook.__iter__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    248\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    249\u001B[0m     it \u001B[38;5;241m=\u001B[39m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__iter__\u001B[39m()\n\u001B[0;32m--> 250\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m obj \u001B[38;5;129;01min\u001B[39;00m it:\n\u001B[1;32m    251\u001B[0m         \u001B[38;5;66;03m# return super(tqdm...) will not catch exception\u001B[39;00m\n\u001B[1;32m    252\u001B[0m         \u001B[38;5;28;01myield\u001B[39;00m obj\n\u001B[1;32m    253\u001B[0m \u001B[38;5;66;03m# NB: except ... [ as ...] breaks IPython async KeyboardInterrupt\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/pytorch_mps/lib/python3.11/site-packages/tqdm/std.py:1181\u001B[0m, in \u001B[0;36mtqdm.__iter__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1178\u001B[0m time \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_time\n\u001B[1;32m   1180\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 1181\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m obj \u001B[38;5;129;01min\u001B[39;00m iterable:\n\u001B[1;32m   1182\u001B[0m         \u001B[38;5;28;01myield\u001B[39;00m obj\n\u001B[1;32m   1183\u001B[0m         \u001B[38;5;66;03m# Update and possibly print the progressbar.\u001B[39;00m\n\u001B[1;32m   1184\u001B[0m         \u001B[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/pytorch_mps/lib/python3.11/site-packages/torch/utils/data/dataloader.py:631\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    628\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    629\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[1;32m    630\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[0;32m--> 631\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_data()\n\u001B[1;32m    632\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m    633\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    634\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    635\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called:\n",
      "File \u001B[0;32m~/anaconda3/envs/pytorch_mps/lib/python3.11/site-packages/torch/utils/data/dataloader.py:675\u001B[0m, in \u001B[0;36m_SingleProcessDataLoaderIter._next_data\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    673\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    674\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[0;32m--> 675\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_fetcher\u001B[38;5;241m.\u001B[39mfetch(index)  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[1;32m    676\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory:\n\u001B[1;32m    677\u001B[0m         data \u001B[38;5;241m=\u001B[39m _utils\u001B[38;5;241m.\u001B[39mpin_memory\u001B[38;5;241m.\u001B[39mpin_memory(data, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory_device)\n",
      "File \u001B[0;32m~/anaconda3/envs/pytorch_mps/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:54\u001B[0m, in \u001B[0;36m_MapDatasetFetcher.fetch\u001B[0;34m(self, possibly_batched_index)\u001B[0m\n\u001B[1;32m     52\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     53\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n\u001B[0;32m---> 54\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcollate_fn(data)\n",
      "File \u001B[0;32m~/anaconda3/envs/pytorch_mps/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py:277\u001B[0m, in \u001B[0;36mdefault_collate\u001B[0;34m(batch)\u001B[0m\n\u001B[1;32m    216\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdefault_collate\u001B[39m(batch):\n\u001B[1;32m    217\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    218\u001B[0m \u001B[38;5;124;03m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001B[39;00m\n\u001B[1;32m    219\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    275\u001B[0m \u001B[38;5;124;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001B[39;00m\n\u001B[1;32m    276\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 277\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m collate(batch, collate_fn_map\u001B[38;5;241m=\u001B[39mdefault_collate_fn_map)\n",
      "File \u001B[0;32m~/anaconda3/envs/pytorch_mps/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py:129\u001B[0m, in \u001B[0;36mcollate\u001B[0;34m(batch, collate_fn_map)\u001B[0m\n\u001B[1;32m    127\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(elem, collections\u001B[38;5;241m.\u001B[39mabc\u001B[38;5;241m.\u001B[39mMapping):\n\u001B[1;32m    128\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 129\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m elem_type({key: collate([d[key] \u001B[38;5;28;01mfor\u001B[39;00m d \u001B[38;5;129;01min\u001B[39;00m batch], collate_fn_map\u001B[38;5;241m=\u001B[39mcollate_fn_map) \u001B[38;5;28;01mfor\u001B[39;00m key \u001B[38;5;129;01min\u001B[39;00m elem})\n\u001B[1;32m    130\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m:\n\u001B[1;32m    131\u001B[0m         \u001B[38;5;66;03m# The mapping type may not support `__init__(iterable)`.\u001B[39;00m\n\u001B[1;32m    132\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m {key: collate([d[key] \u001B[38;5;28;01mfor\u001B[39;00m d \u001B[38;5;129;01min\u001B[39;00m batch], collate_fn_map\u001B[38;5;241m=\u001B[39mcollate_fn_map) \u001B[38;5;28;01mfor\u001B[39;00m key \u001B[38;5;129;01min\u001B[39;00m elem}\n",
      "File \u001B[0;32m~/anaconda3/envs/pytorch_mps/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py:129\u001B[0m, in \u001B[0;36m<dictcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m    127\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(elem, collections\u001B[38;5;241m.\u001B[39mabc\u001B[38;5;241m.\u001B[39mMapping):\n\u001B[1;32m    128\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 129\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m elem_type({key: collate([d[key] \u001B[38;5;28;01mfor\u001B[39;00m d \u001B[38;5;129;01min\u001B[39;00m batch], collate_fn_map\u001B[38;5;241m=\u001B[39mcollate_fn_map) \u001B[38;5;28;01mfor\u001B[39;00m key \u001B[38;5;129;01min\u001B[39;00m elem})\n\u001B[1;32m    130\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m:\n\u001B[1;32m    131\u001B[0m         \u001B[38;5;66;03m# The mapping type may not support `__init__(iterable)`.\u001B[39;00m\n\u001B[1;32m    132\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m {key: collate([d[key] \u001B[38;5;28;01mfor\u001B[39;00m d \u001B[38;5;129;01min\u001B[39;00m batch], collate_fn_map\u001B[38;5;241m=\u001B[39mcollate_fn_map) \u001B[38;5;28;01mfor\u001B[39;00m key \u001B[38;5;129;01min\u001B[39;00m elem}\n",
      "File \u001B[0;32m~/anaconda3/envs/pytorch_mps/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py:121\u001B[0m, in \u001B[0;36mcollate\u001B[0;34m(batch, collate_fn_map)\u001B[0m\n\u001B[1;32m    119\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m collate_fn_map \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    120\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m elem_type \u001B[38;5;129;01min\u001B[39;00m collate_fn_map:\n\u001B[0;32m--> 121\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m collate_fn_map[elem_type](batch, collate_fn_map\u001B[38;5;241m=\u001B[39mcollate_fn_map)\n\u001B[1;32m    123\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m collate_type \u001B[38;5;129;01min\u001B[39;00m collate_fn_map:\n\u001B[1;32m    124\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(elem, collate_type):\n",
      "File \u001B[0;32m~/anaconda3/envs/pytorch_mps/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py:174\u001B[0m, in \u001B[0;36mcollate_tensor_fn\u001B[0;34m(batch, collate_fn_map)\u001B[0m\n\u001B[1;32m    172\u001B[0m     storage \u001B[38;5;241m=\u001B[39m elem\u001B[38;5;241m.\u001B[39m_typed_storage()\u001B[38;5;241m.\u001B[39m_new_shared(numel, device\u001B[38;5;241m=\u001B[39melem\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[1;32m    173\u001B[0m     out \u001B[38;5;241m=\u001B[39m elem\u001B[38;5;241m.\u001B[39mnew(storage)\u001B[38;5;241m.\u001B[39mresize_(\u001B[38;5;28mlen\u001B[39m(batch), \u001B[38;5;241m*\u001B[39m\u001B[38;5;28mlist\u001B[39m(elem\u001B[38;5;241m.\u001B[39msize()))\n\u001B[0;32m--> 174\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mstack(batch, \u001B[38;5;241m0\u001B[39m, out\u001B[38;5;241m=\u001B[39mout)\n",
      "\u001B[0;31mRuntimeError\u001B[0m: stack expects each tensor to be equal size, but got [193] at entry 0 and [467] at entry 1"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "cell_type": "markdown",
   "id": "edcd2c5784bac4f6",
   "metadata": {},
   "source": [
    "### Retrieving the Results\n",
    "\n",
    "Lastly, we will inspect the results of this 'experiment'. Think about some of the caveats, and how to addres them in code (don't worry, the HW does not have (all) caveats), but it is good to be aware of them!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59d0cdcc31ab4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_evaluation(Y_hat, Y):\n",
    "    flat_predictions = list(itertools.chain(*Y_hat))\n",
    "    flat_labels = list(itertools.chain(*Y))\n",
    "    \n",
    "    label_lut = defaultdict(lambda: -1, {'positive': 1, 'negative': 0})\n",
    "    predictions = list(map( lambda x: label_lut[x.split(' ')[0].lower()], flat_predictions))\n",
    "    \n",
    "    accuracy = sum(map(lambda x: x[0] == x[1], zip(predictions, flat_labels))) / len(flat_labels)\n",
    "    unknown = sum(map(lambda x: x[0] == -1, zip(predictions, flat_labels))) / len(flat_labels)\n",
    "    \n",
    "    return accuracy, unknown\n",
    "\n",
    "\n",
    "accuracy, unknown = get_evaluation(predictions_list, labels_list)\n",
    "display(\n",
    "    Markdown(\n",
    "        textwrap.dedent(\n",
    "            f\"\"\"\\\n",
    "            | *Accuracy*  | *Unknown*      |\n",
    "            |:------------|:---------------|\n",
    "            | {accuracy}  | {unknown}      |\n",
    "            \"\"\")\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c538e9b9ec66d1c4",
   "metadata": {},
   "source": [
    "# Part 2: Fine-Tuning the Model\n",
    "\n",
    "Next, we will show the basics of performing fine-tuning of the model, herein, we are going to investigate\n",
    "\n",
    "1. Loading the model\n",
    "2. Defining Hyper-Parameters corresponding to the training process\n",
    "3. Perform evaluation with the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d4854c7cca9ce",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "import peft\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from typing import Tuple\n",
    "\n",
    "\n",
    "def get_peft_details(model):\n",
    "    trainable_model_params = 0\n",
    "    all_model_params = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_model_params += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_model_params += param.numel()\n",
    "    return  trainable_model_params, all_model_params, (100 * trainable_model_params) / all_model_params\n",
    "\n",
    "\n",
    "\n",
    "def train_model(\n",
    "        peft_model,\n",
    "        peft_training_args,\n",
    "        train_set,\n",
    "        test_set = None,\n",
    ") -> Tuple[transformers.Trainer, peft.PeftModel]:\n",
    "    peft_trainer = transformers.Trainer(\n",
    "        model=peft_model,\n",
    "        args=peft_training_args,\n",
    "        train_dataset=train_set,\n",
    "        eval_dataset=test_set,\n",
    "    )\n",
    "    # Pre-train the model\n",
    "    peft_trainer.train()\n",
    "    return peft_model, peft_trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7beae1ca02849daa",
   "metadata": {},
   "source": [
    "## Step 1: Prepare Data and Model\n",
    "\n",
    "Let's continue by preparing a model, note that a lot of this is 'boiler-plate', to reduce the trainign time considerably.\n",
    "\n",
    "If you are interested, and/or want to apply this to your project, we recommend looking into (HF tutorials of) the following:\n",
    "\n",
    "1. Quantization, where and how to apply it. For fine-tuning, we use this in the `LoraConfig`, or Low-Rank Adaptation config, which approximates the full-rank of the gradient with a lower-rank decomposition, thereby considerably reducign the overehad brought by the back-propagation\n",
    "2. Data-types, and when to use them; besides working well for training, inference may also benefit from quantization. In general, experiments are run in 'half' precision (`torch.float16` or `torch.bfloat16`), but lower preicsion exists as well (as low as 1 bit (XOR-quantization))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e156cc1a514e18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # In case we have already defined the peft-model, we remove it.\n",
    "    del peft_model, model\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    model = T5ForConditionalGeneration.from_pretrained(\n",
    "    pretrained_model_name_or_path=model_name,\n",
    "    device_map=device,\n",
    "    torch_dtype=dtype,\n",
    "    )\n",
    "except:\n",
    "    pass\n",
    "# Example of hyper-parameters.\n",
    "RANK = 16               # Rank used in model update (lower is faster, less precise)\n",
    "ALPHA = 32\n",
    "# Scaling factor for update (∆W x dy ALPHA/RANK)           \n",
    "DROPOUT = 0.05          # Regularization term\n",
    "TRAIN_BATCH_SIZE = 32   # Number of samples\n",
    "TRAIN_EPOCHS = 5        # Total number of training steps.\n",
    "LR=5e-4\n",
    "# If you want to save some time, you can store checkpoints, and load them, to create multiple levels\n",
    "# in a single run. Do note, that huggingface by default uses learning-rate scheduling, so this may\n",
    "# affect your results a bit.\n",
    "\n",
    "# The modules are specific to the model itself.\n",
    "MODULES = None # ['k', 'v'] # 'or any other identifier of weights.\n",
    "TORCH_DTYPE = torch.bfloat16\n",
    "\n",
    "# TODO: Decide the levels for your experiment. These can be any of the \n",
    "# aforementioned parameters, or any other hyper-parameter.\n",
    "lora_config = LoraConfig(\n",
    "    r=RANK,\n",
    "    lora_alpha=ALPHA,\n",
    "    target_modules=MODULES,\n",
    "    lora_dropout=DROPOUT,\n",
    "    bias='none',\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM\n",
    ")\n",
    "peft_model = get_peft_model(\n",
    "    model=model,\n",
    "    peft_config=lora_config,\n",
    ")\n",
    "\n",
    "# Define training parameters\n",
    "output_dir = 'llm_lab/t5-small'\n",
    "\n",
    "train_config = transformers.TrainingArguments(\n",
    "    output_dir='./tutorial-3',\n",
    "    per_device_train_batch_size=100,\n",
    "    do_train=True,\n",
    "    do_eval=False,\n",
    "    learning_rate=LR,\n",
    "    # num_train_epochs=TRAIN_EPOCHS,\n",
    "    num_train_epochs=1,\n",
    "    max_steps=100,\n",
    "    save_strategy='steps',\n",
    "    save_steps=500,\n",
    "    logging_strategy='steps',\n",
    "    logging_steps=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86a347b69f67397",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "pft, orig, pct = get_peft_details(peft_model)\n",
    "display(\n",
    "    Markdown(\n",
    "        textwrap.dedent(\n",
    "            f\"\"\"\\\n",
    "            | Parameter        | Statistic |\n",
    "            |:-----------------|:----------|\n",
    "            | Original         | {orig}    |\n",
    "            | PEFT             | {pft}     |\n",
    "            | Percentage       | {pct:.2f}%|\n",
    "            \"\"\"\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2e5beba93ce2e0",
   "metadata": {},
   "source": [
    "## Step 2: And Lift-off (ish)\n",
    "Let's do a round of training, and look at'er go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf06fc9e9eb9285",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some helper functions\n",
    "def limit_by_tokens(batch, max_length: int = 150, tokenizer: transformers.AutoTokenizer = tokenizer):\n",
    "    \"\"\"Helper method to limit the string length using the user defined tokenizer\n",
    "    \n",
    "    Args:\n",
    "        batch (LazyBatch): Batch to tokenizer.\n",
    "    \n",
    "    Keyword Args:\n",
    "        max_length (int): maximum lenght to tokenize data to, defaults to 150.\n",
    "        tokenizer (transformers.AutoTokenizer, optional): Tokenizer of the model to use.\n",
    "        \n",
    "    Returns:\n",
    "        LazyBatch: Batch with limited string length in string fromat.\n",
    "    \"\"\"\n",
    "    token_repr = tokenizer.batch_encode_plus(batch['text'], max_length=max_length, padding=True, truncation=True, add_special_tokens=False)['input_ids']\n",
    "    short_string = tokenizer.batch_decode(token_repr, skip_special_tokens=True)\n",
    "    batch['text'] = short_string\n",
    "    return batch\n",
    "\n",
    "def tokenize_function(\n",
    "        batch,\n",
    "        prefix='Is the following Positive or Negative?\\n',\n",
    "        post_fix='\\nAnswer: '):\n",
    "\n",
    "    updated_text = [f\"{prefix}{review}{post_fix}\" for review in batch[\"text\"]]\n",
    "    batch['text'] = updated_text\n",
    "    tokenized_text = tokenizer.batch_encode_plus(updated_text, padding=True)\n",
    "    batch['input_ids'] = tokenized_text.input_ids\n",
    "    batch['attention_mask'] = tokenized_text.attention_mask\n",
    "    # We also set the 'response', i.e., what the model should learn\n",
    "    batch['labels'] = tokenizer.batch_encode_plus(['Positive' if label == 1 else 'Negative' for label in batch[\"label\"]], padding='max_length', truncation=True, max_length=3, return_tensors=\"pt\").input_ids\n",
    "\n",
    "    return batch\n",
    "\n",
    "# If the number of tokens is a level, you might need to change this\n",
    "train_ready_set = (\n",
    "    train_set\n",
    "    .map(limit_by_tokens, batched=True, num_proc=10, batch_size=200)\n",
    "    .map(tokenize_function, batched=True)\n",
    ")\n",
    "example = train_ready_set[0]\n",
    "newline = '\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9a6dbe4a855557",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample\n",
    "display(\n",
    "    Markdown(\n",
    "        textwrap.dedent(\n",
    "            f\"\"\"\\\n",
    "            | *Key*             | *Example*              |\n",
    "            |-------------------|------------------------|\n",
    "            | *text*            | {example['text'].replace(newline, '<br>')}       |\n",
    "            | *input_ids*      | {example['input_ids']}       |\n",
    "            | *labels*         | {example['labels']}       |\n",
    "            | *detokenized_labels*| {tokenizer.decode(example['labels'])}   |\n",
    "            \"\"\"\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41471c6eb0dbd085",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate with original model.\n",
    "\n",
    "test_dataset = (\n",
    "    test_set\n",
    "    .map(limit_by_tokens, batched=True, num_proc=10, batch_size=200)\n",
    "    .map(\n",
    "        tokenize_function, batched=True\n",
    "    )\n",
    "    .map(\n",
    "        lambda batch: tokenizer.batch_encode_plus(\n",
    "            batch['text'],\n",
    "            add_special_tokens=True,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=False,\n",
    "        ), batched=True\n",
    "    )\n",
    ")\n",
    "# Ensure we can effectively use the model\n",
    "test_dataset.set_format(type='torch', columns=['input_ids', 'label', 'attention_mask'])\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "    dataset=test_dataset, # .shuffle(seed=123).take(2000),\n",
    "    batch_size=400,  # Feel free to lower / higher this\n",
    "    shuffle=False,  # Shuffling not needed during evaluation\n",
    "    num_workers=5,  # Feel free to set this to -1 or 1, esp. on CPU \n",
    "    prefetch_factor=200,\n",
    ")\n",
    "\n",
    "with torch.no_grad():\n",
    "    result, labels = evaluate_fn(test_dataloader, evaluate_model=model)\n",
    "accuracy, unknown = get_evaluation(result, labels)\n",
    "display(\n",
    "    Markdown(\n",
    "        textwrap.dedent(\n",
    "            f\"\"\"\\\n",
    "            | *Accuracy*  | *Unknown*      |\n",
    "            |:------------|:---------------|\n",
    "            | {accuracy}  | {unknown}      |\n",
    "            \"\"\")\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4179dc6b36b4e5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "peft_model, peft_trainer = train_model(\n",
    "    peft_model=peft_model,\n",
    "    peft_training_args=train_config,\n",
    "    train_set=train_ready_set,\n",
    "    test_set=None,\n",
    ")\n",
    "peft_model.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db091d66fe793423",
   "metadata": {},
   "source": [
    "### Step 2.1 Let's evaluate the model...\n",
    "\n",
    "Can you think fo some caveats fo the model, what would happen if:\n",
    "\n",
    "1. We change the prompt after training?\n",
    "2. We change the input length of the model?\n",
    "3. We want to include additional sentiments, such as `neutral`?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b259008d1cafa46",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with torch.no_grad():\n",
    "    peft_model.eval()\n",
    "    result, labels = evaluate_fn(test_dataloader, evaluate_model=peft_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbc9b48d551094b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "accuracy, unknown = get_evaluation(result, labels)\n",
    "display(\n",
    "    Markdown(\n",
    "        textwrap.dedent(\n",
    "            f\"\"\"\\\n",
    "            ### Results after PEFT fine-tuning\n",
    "            | *Accuracy*  | *Unknown*      |\n",
    "            |:------------|:---------------|\n",
    "            | {accuracy}  | {unknown}      |\n",
    "            \"\"\"\n",
    "        )\n",
    "    ),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94e57ef3941610a",
   "metadata": {},
   "source": [
    "# Part 3: Few-Shot inference\n",
    "\n",
    "During the last session we will cover running few-shot learning, a derivative from the 'Zero-Shot' prompting, that we have seen before.\n",
    "\n",
    "In contrast to zero-shot learning, we provide some 'examples' to the model, consisting of a 'Review' and a classification of 'Sentiment' (Positive/Negative). Consecutively, we prompt the model to classify the actual piece of information that we are interested in. As such, this consists of the following parts;\n",
    "\n",
    "1. Selecting *which* examples to use,\n",
    "2. Selecting *how to represent the examples,\n",
    "3. Pre-processing the data (similar to what we've seen before).\n",
    "4. Do inference.\n",
    "\n",
    "\n",
    "Generally, in few-shot, we can control the number of examples, i.e., shots, with a parameter $k$. So a k-shot example. Throughout this example, we will focus on a $k$ with 2 Positive and 2 Negative examples. However, in your experiments, you may wish to control $k$, or even de split of Positive and Negative examples that you will provide to the model.\n",
    "\n",
    "> The goal of this part is to show how you can create a flexible template, apply it to the data, and evaluate few-shot experiments effectively.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc4d8ac03aadb5",
   "metadata": {},
   "source": [
    "## Step 1: Selecting Few-Shot Examples\n",
    "\n",
    "First, we will need to select a few simple examples, for this we need to think about to sample from where, and to make sure that we can select relevant samples.\n",
    "\n",
    "Let us first define some helper functions to use durin gthis part of the lab.\n",
    "\n",
    "**N.B.**, some of these functions are redundant, but that is to prevent to have to 'cherry pick' the cells from above that\n",
    "we want to run. Alternatively, you may want to choose to write a small library to import helper functions from a single,\n",
    "file, but this is out of the scope of this course.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f3e2dada53315b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some helper functions\n",
    "\n",
    "def sentiment_split_dataset(dataset: datasets.Dataset):\n",
    "    \"\"\"Helper function to split a dataset into a positive and negative set using their provided label.\n",
    "    \n",
    "    Args:\n",
    "        dataset (dataset.Dataset): Dataset (split) to seperate into positive and negative datasets.\n",
    "    \"\"\"\n",
    "    \n",
    "    positive_split = dataset.filter(lambda sample: sample['label'] == 1)\n",
    "    negative_split = dataset.filter(lambda sample: sample['label'] == 0)\n",
    "    \n",
    "    return positive_split, negative_split\n",
    "\n",
    "def limit_by_tokens(batch, max_length: int = 150, tokenizer: transformers.AutoTokenizer = None):\n",
    "    \"\"\"Helper method to limit the string length using the user defined tokenizer\n",
    "    \n",
    "    Args:\n",
    "        batch (LazyBatch): Batch to tokenizer.\n",
    "    \n",
    "    Keyword Args:\n",
    "        max_length (int): maximum lenght to tokenize data to, defaults to 150.\n",
    "        tokenizer (transformers.AutoTokenizer, optional): Tokenizer of the model to use.\n",
    "        \n",
    "    Returns:\n",
    "        LazyBatch: Batch with limited string length in string fromat.\n",
    "    \"\"\"\n",
    "    token_repr = tokenizer.batch_encode_plus(batch['text'], max_length=max_length, padding=True, truncation=True, add_special_tokens=False)['input_ids']\n",
    "    short_string = tokenizer.batch_decode(token_repr, skip_special_tokens=True)\n",
    "    batch['text'] = short_string\n",
    "    return batch\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_fn(data_loader, evaluate_model) -> Tuple[List[List[str]], List[List[int]]]:\n",
    "    \"\"\"Evaluation of dataloader and model, note that deterministic (`do_sampling=False`) is used, as we ask the model\n",
    "    to classify positive and negative examples.\n",
    "    \n",
    "    Args:\n",
    "        data_loader (DataLoader): Dataloader containing evaluation data.\n",
    "        evaluate_model (transformers.AutoModelForSeq2SeqLM): Model used for evaluation.\n",
    "        \n",
    "    Returns:\n",
    "        List of lists of strings containing the model's prediction.\n",
    "        List of lists of integers representing the ground truth labels.\n",
    "    \"\"\"\n",
    "    labels_list = []\n",
    "    prediction_list = []\n",
    "    for batch in tqdm(data_loader):\n",
    "        input_ids, attention_mask, label = batch['input_ids'].to(evaluate_model.device), batch['attention_mask'].to(evaluate_model.device), batch['label'].to(evaluate_model.device)\n",
    "        outputs = evaluate_model.generate(\n",
    "          input_ids=input_ids,\n",
    "          attention_mask=attention_mask,\n",
    "          max_length=3,\n",
    "          do_sample=False,\n",
    "        )\n",
    "        prediction = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "        prediction_list.append(prediction)\n",
    "        labels_list.append(label.tolist())\n",
    "    return prediction_list, labels_list\n",
    "\n",
    "\n",
    "def get_evaluation(Y_hat: List[List[str]], Y: List[List[int]]):\n",
    "    \"\"\"Helper function to perform evaluation.\n",
    "    \n",
    "    Args:\n",
    "        Y_hat (List[List[str]]): Minibatched prediction results (str) of a language model.\n",
    "        Y (List[List[int]]): Minibatched ground-truth labels (int) of a dataset.\n",
    "\n",
    "    Returns:\n",
    "        float representing the accuracy of the model.\n",
    "        float representing the fraction of data for which neither positive or negative was given.\n",
    "    \"\"\"\n",
    "    flat_predictions = list(itertools.chain(*Y_hat))\n",
    "    flat_labels = list(itertools.chain(*Y))\n",
    "\n",
    "    label_lut = defaultdict(lambda: -1, {'positive': 1, 'negative': 0})\n",
    "    predictions = list(map(lambda x: label_lut[x.split(' ')[0].lower()], flat_predictions))\n",
    "\n",
    "    accuracy = sum(map(lambda x: x[0] == x[1], zip(predictions, flat_labels))) / len(flat_labels)\n",
    "    unknown = sum(map(lambda x: x[0] == -1, zip(predictions, flat_labels))) / len(flat_labels)\n",
    "\n",
    "    return accuracy, unknown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5021a0d0d84e60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "positive_dataset, negative_dataset = sentiment_split_dataset(train_set)\n",
    "k_shots = 4\n",
    "shots = []\n",
    "labels = []\n",
    "# TODO: select $k$ samples, and create a list of 'shots' and 'labels'\n",
    "# YOUR CODE GOES HERE\n",
    "...    \n",
    "# END OF YOUR CODE\n",
    "\n",
    "# Here, we combine the labels and shots to get a single result\n",
    "few_shots = list(zip(shots, labels))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48db798e032558a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper definition to render the results (truncated)...\n",
    "newline = '\\n'\n",
    "display(\n",
    "    Markdown('### Selected shots: '),\n",
    "    Markdown(textwrap.dedent(\n",
    "        f\"\"\"\\\n",
    "        | Shot          |       Review          | Sentiment     |\n",
    "        |---------------|-----------------------|---------------|\n",
    "        {\"        \".join(f\"| {idx} | {shot[:100].replace(newline, '<br>')}... | {label} |{newline}\" for idx, (shot, label) in enumerate(few_shots))}\"\"\")\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e57b1e5b11cc04c",
   "metadata": {},
   "source": [
    "## Step 2: Preparing the Few-Shot Prompt\n",
    "\n",
    "Next, we need to design a simple approach to make sure we can represent the data to the model, effectively, we want to be able to provide the model with something like the following:\n",
    "\n",
    "\n",
    "> Context:\n",
    ">  Review: I liked the dramatic opening of the first scene.\\\n",
    ">  Sentiment: Positive\n",
    ">\n",
    ">  Review: That actor makes me want to throw my phone away.\\\n",
    ">  Sentiment: Negative\n",
    ">    \n",
    ">  Question: Is the following positive or negative?\\\n",
    ">  Review: I like turtles.\\\n",
    ">  Sentiment:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5fbf3cca0df23d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jinja2\n",
    "import textwrap\n",
    "\n",
    "template = jinja2.Template(\n",
    "    textwrap.dedent(\n",
    "        ### YOUR CODE GOES HERE\n",
    "        ...\n",
    "        ### END OF YOUR CODE\n",
    "    )\n",
    ")\n",
    "\n",
    "example_shots: List[Tuple[str]] = [('Well, hello there.', 'Positive'), ('I like turtles', 'Positive'), (\"...\", 'Negative'), ('Whelp, I cannot think of other small example reviews', 'Negative')]\n",
    "\n",
    "review: str = 'De kat krabt de krullen van de trap.'\n",
    "example_prompt: str = template.render(shots=example_shots, review=review).replace('\\n', '<br>')\n",
    "\n",
    "display(\n",
    "    Markdown(\"### Example of your rendered template\"),\n",
    "    Markdown(f\"> {example_prompt}\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cec0b7a6a88c3c6",
   "metadata": {},
   "source": [
    "## Step 3: Preparing the data\n",
    "\n",
    "Next, we will pre-process the data as we have seen before, here we ensure:\n",
    "\n",
    "1. We render the input using the `Template` we just designed.\n",
    "2. We provide the appropriate examples that we selected earlier.\n",
    "3. We tokenize the data.\n",
    "\n",
    "To speed up this part, we have collated these steps in a single function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b93ab84844d048",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def process_few_shot_batch(batch, template: jinja2.Template, shots: List[Tuple[str, str]], tokenizer = None, randomize=False):\n",
    "    \"\"\"Simple all-in-one render and tokenization method to render few-shots examples.\n",
    "    \n",
    "    Args:\n",
    "        batch (LazyBatch): batch to render as a few-shot sample.\n",
    "        template (Template): Jinja template to use to render few-shot examples.\n",
    "        shots (List[Tuple[str, str]]): List of shots to render.\n",
    "        tokenizer (Tokenizer): Model tokenizer.    \n",
    "    \"\"\"\n",
    "    rendered_samples = [template.render(review=sample, shots=random.sample(shots, k=len(shots))) for sample in batch['text']]\n",
    "    if tokenizer is not None:\n",
    "        return tokenizer.batch_encode_plus(rendered_samples, padding=True, add_special_tokens=True, truncation=False, return_tensors='pt')\n",
    "    else:\n",
    "        batch['text'] = rendered_samples\n",
    "        return batch\n",
    "\n",
    "few_shot_evaluation = (\n",
    "    test_set\n",
    "    .shuffle(seed=421)\n",
    "    .take(2000)\n",
    "    .map(limit_by_tokens, batched=True, num_proc=10, batch_size=200)\n",
    "    .map(lambda batch: process_few_shot_batch(batch, template, few_shots, tokenizer), batched=True)\n",
    ")\n",
    "# And set the dataset format to something expected. :)\n",
    "few_shot_evaluation.set_format(type='torch', columns=['input_ids', 'label', 'attention_mask'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945077151632c272",
   "metadata": {},
   "source": [
    "## Step 4: Running inference.\n",
    "\n",
    "Next, we run the inference on the model, to see how the model improves over the prior art."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6b8017300111c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate with original model.\n",
    "\n",
    "# N.B., We set the batch size lower, as we increase the lenght of the input prompt.\n",
    "few_shot_loader = torch.utils.data.DataLoader(\n",
    "    dataset=few_shot_evaluation, # .shuffle(seed=123).take(2000),\n",
    "    batch_size=50,  # Feel free to lower / higher this\n",
    "    shuffle=False,  # Shuffling not needed during evaluation\n",
    "    num_workers=2,  # Feel free to set this to -1 or 1, esp. on CPU \n",
    "    prefetch_factor=50,\n",
    ")\n",
    "\n",
    "with torch.no_grad():\n",
    "    result, labels = evaluate_fn(few_shot_loader, evaluate_model=model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8abea33cd73c54ba",
   "metadata": {},
   "source": [
    "## Step 5: Looking at the results\n",
    "\n",
    "Lastly, we evaluate the results, can you think of ways to further improve the results? What would happen if we:\n",
    "\n",
    "1. Would take different few-shot examples?\n",
    "2. Would take more few-shot examples?\n",
    "3. Take different numbers of Positive/Negative samples?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf82bba09eb8601",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy, unknown = get_evaluation(result, labels)\n",
    "display(\n",
    "    Markdown(\n",
    "        textwrap.dedent(\n",
    "            f\"\"\"\\\n",
    "            | *Accuracy*  | *Unknown*      |\n",
    "            |:------------|:---------------|\n",
    "            | {accuracy}  | {unknown}      |\n",
    "            \"\"\")\n",
    "    ),\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
